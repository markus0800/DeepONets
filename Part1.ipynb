{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, vmap, jit, grad, value_and_grad\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from functools import partial\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, layer_sizes, key, activation_fn=jax.nn.tanh):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.params = self.initialize_params(layer_sizes, key)\n",
    "\n",
    "    def initialize_params(self, layer_sizes, key):\n",
    "        params = []\n",
    "        keys = random.split(key, len(layer_sizes) - 1)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W_key, b_key = random.split(keys[i])\n",
    "            # Xavier initialization for weights\n",
    "            in_dim = layer_sizes[i]\n",
    "            out_dim = layer_sizes[i+1]\n",
    "            W = random.uniform(W_key, (in_dim, out_dim), minval=-jnp.sqrt(6 / (in_dim + out_dim)), maxval=jnp.sqrt(6 / (in_dim + out_dim)))\n",
    "            # Initialize biases with zeros\n",
    "            b = jnp.zeros(out_dim)\n",
    "            params.append((W, b))\n",
    "        return params\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward(self, params, x):\n",
    "        for W, b in params[:-1]:\n",
    "            # Linear transformation\n",
    "            x = jnp.dot(x, W) + b\n",
    "            # Apply activation function\n",
    "            x = self.activation_fn(x)\n",
    "        # Output layer (no activation function)\n",
    "        W, b = params[-1]\n",
    "        x = jnp.dot(x, W) + b\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predict output for input x\n",
    "        return vmap(self.forward, in_axes=(None, 0))(self.params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepONet:\n",
    "    def __init__(self, branch_layer_sizes, trunk_layer_sizes, key, activation_fn=jax.nn.tanh):\n",
    "        # Initialize branch and trunk networks\n",
    "        branch_key, trunk_key = random.split(key)\n",
    "        self.branch_net = FeedForwardNN(branch_layer_sizes, branch_key, activation_fn)\n",
    "        self.trunk_net = FeedForwardNN(trunk_layer_sizes, trunk_key, activation_fn)\n",
    "\n",
    "    def params(self):\n",
    "        return [self.branch_net.params, self.trunk_net.params]\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward(self, params, branch_input, trunk_input):\n",
    "        # Forward pass through branch and trunk networks\n",
    "        branch_output = self.branch_net.forward(params[0], branch_input)\n",
    "        trunk_output = self.trunk_net.forward(params[1], trunk_input)\n",
    "        # Combine outputs using inner product\n",
    "        return jnp.dot(branch_output, trunk_output.T)\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward_squeeze(self, params, branch_input, trunk_input):\n",
    "        return self.forward(params, branch_input, trunk_input).squeeze()\n",
    "\n",
    "    def predict(self, branch_input, trunk_input):\n",
    "        # Predict output for given inputs\n",
    "        return vmap(self.forward, in_axes=(None, 0, 0))(self.params(), branch_input, trunk_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameter omega\n",
    "def generate_omega(omega_min, omega_max, n_samples, key):\n",
    "    \n",
    "    # # Normally distributed omega\n",
    "\n",
    "    # Parameters for the true normal distribution\n",
    "    mean = (omega_max + omega_min) / 2\n",
    "    std = 2  # Smaller std = tighter around center\n",
    "\n",
    "    # Rejection sampling: draw until we have enough inside bounds\n",
    "    samples = []\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        # Use JAX's random.normal instead of NumPy's\n",
    "        proposed = random.normal(key, shape=(n_samples,)) * std + mean\n",
    "        proposed = np.array(proposed)  # Convert to NumPy for boolean masking\n",
    "        accepted = proposed[(proposed >= omega_min) & (proposed <= omega_max)]\n",
    "        samples.extend(accepted.tolist())\n",
    "        key, _ = random.split(key)  # Update key for next batch\n",
    "\n",
    "    omega_normal = np.array(samples[:n_samples])\n",
    "\n",
    "    # # Uniformly distributed omega\n",
    "\n",
    "    omega_uniform = random.uniform(\n",
    "        key, shape=(n_samples,), minval=omega_min, maxval=omega_max\n",
    "    )\n",
    "\n",
    "    # # Beta distributed omega\n",
    "\n",
    "    # Parameters for the beta distribution\n",
    "    alpha = 0.4\n",
    "    beta = 0.4\n",
    "\n",
    "    # Generate beta-distributed samples in [0, 1]\n",
    "    beta_samples = random.beta(key, alpha, beta, shape=(n_samples,))\n",
    "\n",
    "    # Scale the beta samples to the desired range [omega_min, omega_max]\n",
    "    omega_beta = omega_min + (omega_max - omega_min) * beta_samples\n",
    "\n",
    "    return omega_normal, omega_uniform, omega_beta\n",
    "\n",
    "omega_min = -1.0\n",
    "omega_max = 3 * np.pi\n",
    "\n",
    "random_key = random.PRNGKey(42)  # Random key for JAX\n",
    "n_samples = 5000\n",
    "omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "\n",
    "# --- Plot all three histograms in one figure ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histograms with histtype='step' (outline only)\n",
    "plt.hist(\n",
    "    omega_normal, bins=50, density=True, histtype=\"step\", linewidth=2, label=\"Normal\"\n",
    ")\n",
    "plt.hist(\n",
    "    omega_uniform, bins=50, density=True, histtype=\"step\", linewidth=2, label=\"Uniform\"\n",
    ")\n",
    "plt.hist(\n",
    "    omega_beta,\n",
    "    bins=50,\n",
    "    density=True,\n",
    "    histtype=\"step\",\n",
    "    linewidth=2,\n",
    "    label=\"Beta(0.4, 0.4)\",\n",
    ")\n",
    "\n",
    "plt.title(\"Comparison of Omega Distributions\", fontsize=14)\n",
    "plt.xlabel(\"Omega\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate random x values between -1 and 1 with a different random key\n",
    "key_x = random.PRNGKey(2)\n",
    "x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "# Compute y values for the generated omega and x\n",
    "y_normal = jnp.sin(omega_normal * x)\n",
    "\n",
    "y_uniform = jnp.sin(omega_uniform * x)\n",
    "\n",
    "y_beta = jnp.sin(omega_beta * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_training_data(omega, x, y, n_batch, iter):\n",
    "    \"\"\"\n",
    "    Samples a batch of training data from x and y using the same random indices.\n",
    "\n",
    "    Args:\n",
    "        x (jnp.ndarray): Input vector.\n",
    "        y (jnp.ndarray): Target vector.\n",
    "        n_batch (int): Batch size.\n",
    "        iter (int): Current iteration (used to generate a random seed).\n",
    "\n",
    "    Returns:\n",
    "        x_batch (jnp.ndarray): Sampled input batch.\n",
    "        y_batch (jnp.ndarray): Sampled target batch.\n",
    "    \"\"\"\n",
    "    # Generate a random key based on the iteration\n",
    "    key = random.PRNGKey(iter)\n",
    "    \n",
    "    # Generate random indices for sampling\n",
    "    indices = random.choice(key, len(x), shape=(n_batch,), replace=False)\n",
    "    \n",
    "    # Sample x and y using the same indices\n",
    "    omega_batch = omega[indices]\n",
    "    x_batch = x[indices]\n",
    "    y_batch = y[indices]\n",
    "    \n",
    "    return omega_batch, x_batch, y_batch\n",
    "\n",
    "def initialize_DON(key=None):\n",
    "    # Use the provided key or default to random.PRNGKey(0)\n",
    "    if key is None:\n",
    "        key = random.PRNGKey(0)\n",
    "    \n",
    "    # Create a neural network\n",
    "    branch_layer_sizes = [1, 50, 50, 10]\n",
    "    trunk_layer_sizes = [1, 50, 50, 10]\n",
    "    nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, key)\n",
    "\n",
    "    return nn\n",
    "\n",
    "def train_DON(nn,omega,x,y,max_iter,batch_sampling=False):\n",
    "\n",
    "    def loss_fn(params, omega, x, y):\n",
    "        return jnp.mean(squared_residual_batch(params, omega, x)) + jnp.mean(squared_errors_batch(params, omega, x, y))\n",
    "\n",
    "    # Define training step with Adam optimizer\n",
    "    @jit\n",
    "    def train_step(params, opt_state, omega, x, y):    \n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, omega, x, y)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, opt_state, loss\n",
    "\n",
    "    # Define the loss function (Mean Squared Error)\n",
    "    # Define the loss function\n",
    "    u_x = jit(grad(nn.forward_squeeze, argnums=2))\n",
    "    squared_residual = jit(lambda params, omega, x: (u_x(params, omega, x) - omega * jnp.cos(omega * x)) ** 2)\n",
    "    squared_residual_batch = jit(vmap(squared_residual, in_axes=(None, 0, 0)))\n",
    "\n",
    "    # With or without data\n",
    "    # squared_error = jit(lambda params, omega: nn.forward(params, omega, 0) ** 2)\n",
    "    squared_error = jit(lambda params, omega, x, y: (nn.forward(params, omega, x) - y) ** 2)\n",
    "    squared_errors_batch = jit(vmap(squared_error, in_axes=(None, 0, 0, 0)))\n",
    "    \n",
    "    # Initialize the Adam optimizer with the learning rate schedule\n",
    "    learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "        init_value=0.01,\n",
    "        boundaries_and_scales={300: 0.2, 2000: 0.5}\n",
    "    )\n",
    "    optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "    opt_state = optimizer.init(nn.params())\n",
    "    losses = []\n",
    "\n",
    "    # Training loop\n",
    "    max_iterations = max_iter\n",
    "    batch_ratio = 10\n",
    "\n",
    "    # Training loop with tqdm progress bar with batch sampling\n",
    "    if batch_sampling:\n",
    "        pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "        for i, epoch in enumerate(pbar):\n",
    "            \n",
    "            omega_batch, x_batch, y_batch = sample_training_data(omega, x, y, round(n_samples/batch_ratio), i)\n",
    "\n",
    "            don_params, opt_state, current_loss = train_step(nn.params(), opt_state, omega_batch, x_batch, y_batch)\n",
    "            nn.branch_net.params = don_params[0] \n",
    "            nn.trunk_net.params = don_params[1]\n",
    "            losses.append(current_loss)\n",
    "            if epoch % 100 == 0:\n",
    "                pbar.set_postfix({'loss': current_loss})\n",
    "            if current_loss < 1.0e-5:\n",
    "                break\n",
    "    else:\n",
    "        # Training loop without batch sampling\n",
    "        pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "        for i, epoch in enumerate(pbar):\n",
    "            don_params, opt_state, current_loss = train_step(nn.params(), opt_state, omega, x, y)\n",
    "            nn.branch_net.params = don_params[0] \n",
    "            nn.trunk_net.params = don_params[1]\n",
    "            losses.append(current_loss)\n",
    "            if epoch % 100 == 0:\n",
    "                pbar.set_postfix({'loss': current_loss})\n",
    "            if current_loss < 1.0e-5:\n",
    "                break\n",
    "\n",
    "    return nn, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 5000\n",
    "\n",
    "nn_normal = initialize_DON()\n",
    "\n",
    "nn_normal, losses_normal = train_DON(\n",
    "    nn_normal, omega_normal, x, y_normal, max_iterations\n",
    ")\n",
    "\n",
    "nn_uniform = initialize_DON()\n",
    "\n",
    "nn_uniform, losses_uniform = train_DON(\n",
    "    nn_uniform, omega_uniform, x, y_uniform, max_iterations\n",
    ")\n",
    "\n",
    "nn_beta = initialize_DON()\n",
    "\n",
    "nn_beta, losses_beta = train_DON(nn_beta, omega_beta, x, y_beta, max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_omegas = 50  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "omega_tests = jnp.linspace(omega_min - delta, omega_max + delta, n_test_omegas)\n",
    "\n",
    "mse_normal_results = []\n",
    "mse_uniform_results = []\n",
    "mse_beta_results = []\n",
    "\n",
    "for omega_test in omega_tests:\n",
    "    omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "    y_test = jnp.sin(omega_test_array * x_test)\n",
    "\n",
    "    mse_normal = np.mean(\n",
    "        (y_test - nn_normal.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "        ** 2\n",
    "    )\n",
    "    mse_uniform = np.mean(\n",
    "        (y_test - nn_uniform.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "        ** 2\n",
    "    )\n",
    "    mse_beta = np.mean(\n",
    "        (y_test - nn_beta.predict(omega_test_array, x_test).reshape(y_test.shape)) ** 2\n",
    "    )\n",
    "\n",
    "    mse_normal_results.append(mse_normal)\n",
    "    mse_uniform_results.append(mse_uniform)\n",
    "    mse_beta_results.append(mse_beta)\n",
    "\n",
    "\n",
    "# Plot MSE vs Omega (optional)\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- First subplot: Omega distributions ---\n",
    "ax1.hist(omega_normal, bins=50, density=True, histtype=\"step\", \n",
    "         linewidth=2, label=\"Normal\", color=colors[0])\n",
    "ax1.hist(omega_uniform, bins=50, density=True, histtype=\"step\", \n",
    "         linewidth=2, label=\"Uniform\", color=colors[1])\n",
    "ax1.hist(omega_beta, bins=50, density=True, histtype=\"step\", \n",
    "         linewidth=2, label=\"Beta\", color=colors[2])\n",
    "\n",
    "ax1.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax1.set_ylabel(\"Density\", fontsize=12)\n",
    "ax1.set_title(\"Omega Distributions\", fontsize=14)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# --- Second subplot: MSE results (log scale) ---\n",
    "ax2.semilogy(omega_tests, mse_normal_results, \"o-\", linewidth=2, \n",
    "             markersize=5, label=\"Normal\", color=colors[0])\n",
    "ax2.semilogy(omega_tests, mse_uniform_results, \"o-\", linewidth=2, \n",
    "             markersize=5, label=\"Uniform\", color=colors[1])\n",
    "ax2.semilogy(omega_tests, mse_beta_results, \"o-\", linewidth=2, \n",
    "             markersize=5, label=\"Beta\", color=colors[2])\n",
    "\n",
    "# Add vertical dotted lines at boundaries\n",
    "ax2.axvline(omega_min, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "ax2.axvline(omega_max, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax2.set_ylabel(\"MSE (log scale)\", fontsize=12)\n",
    "ax2.set_title(\"Prediction Error (Log Scale)\", fontsize=14)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, linestyle=\"--\", alpha=0.6, which='both')\n",
    "\n",
    "# Add minor grid lines for the log scale\n",
    "ax2.minorticks_on()\n",
    "ax2.grid(which='minor', linestyle=':', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the results for nn\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(4, 1, figsize=(14, 20))\n",
    "\n",
    "# Plot the loss over the number of iterations for nn\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].plot(range(len(losses_normal)), losses_normal, label=\"Loss (normal)\")\n",
    "axs[0].plot(range(len(losses_uniform)), losses_uniform, label=\"Loss (uniform)\")\n",
    "axs[0].plot(range(len(losses_beta)), losses_beta, label=\"Loss (beta)\")\n",
    "axs[0].set_xlabel(\"Iteration\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the final results for nn\n",
    "omega_tests = jnp.linspace(omega_min, omega_max, 5)\n",
    "# key_uniform = random.PRNGKey(10)\n",
    "# omega_tests = random.uniform(key_uniform, shape=(5,), minval=omega_min, maxval=omega_max)\n",
    "omega_tests = omega_tests.at[1].set(0)\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(10, 6))  # Adjust layout if needed\n",
    "\n",
    "for omega_test, color in zip(omega_tests, colors):\n",
    "    omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "    y_test = jnp.sin(omega_test_array * x_test)\n",
    "    # Plot true sine function (dashed line)\n",
    "    axs[1].plot(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Sine function (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    # Plot neural network prediction (solid line)\n",
    "    axs[1].plot(\n",
    "        x_test,\n",
    "        nn_normal.predict(omega_test_array, x_test).reshape(y_test.shape),\n",
    "        color=color,\n",
    "        label=f\"Neural network (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    axs[2].plot(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Sine function (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    # Plot neural network prediction (solid line)\n",
    "    axs[2].plot(\n",
    "        x_test,\n",
    "        nn_uniform.predict(omega_test_array, x_test).reshape(y_test.shape),\n",
    "        color=color,\n",
    "        label=f\"Neural network (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    axs[3].plot(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Sine function (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    # Plot neural network prediction (solid line)\n",
    "    axs[3].plot(\n",
    "        x_test,\n",
    "        nn_beta.predict(omega_test_array, x_test).reshape(y_test.shape),\n",
    "        color=color,\n",
    "        label=f\"Neural network (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "# Add legend outside the plot\n",
    "axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "axs[2].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "axs[3].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Set titles for each subplot\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[1].set_title(\"Normal\")\n",
    "axs[2].set_title(\"Uniform\") \n",
    "axs[3].set_title(\"Beta\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_omegas = 50  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "omega_tests = jnp.linspace(omega_min - delta, omega_max + delta, n_test_omegas)\n",
    "\n",
    "# Number of repetitions for training\n",
    "num_repeats = 5\n",
    "\n",
    "# Initialize lists to store MSE results for each repetition\n",
    "all_mse_normal_results = []\n",
    "all_mse_uniform_results = []\n",
    "all_mse_beta_results = []\n",
    "\n",
    "n_samples = 500\n",
    "max_iterations = 4000\n",
    "\n",
    "key_adjust = 42\n",
    "\n",
    "for i in range(num_repeats):\n",
    "    print(f\"Training repetition {i + 1}/{num_repeats}\")\n",
    "\n",
    "    random_key = random.PRNGKey(key_adjust + i)  # Random key for JAX\n",
    "    omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "\n",
    "    # Generate random x values between -1 and 1 with a different random key\n",
    "    key_x = random.PRNGKey(key_adjust + i)\n",
    "    x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "    # Compute y values for the generated omega and x\n",
    "    y_normal = jnp.sin(omega_normal * x)\n",
    "\n",
    "    y_uniform = jnp.sin(omega_uniform * x)\n",
    "\n",
    "    y_beta = jnp.sin(omega_beta * x)\n",
    "\n",
    "    # Train DON models\n",
    "    random_key = random.PRNGKey(i)\n",
    "    nn_normal = initialize_DON(key_adjust + random_key)\n",
    "    nn_normal, losses_normal = train_DON(\n",
    "        nn_normal, omega_normal, x, y_normal, max_iterations\n",
    "    )\n",
    "\n",
    "    random_key = random.PRNGKey(key_adjust + num_repeats + i)\n",
    "    nn_uniform = initialize_DON(random_key)\n",
    "    nn_uniform, losses_uniform = train_DON(\n",
    "        nn_uniform, omega_uniform, x, y_uniform, max_iterations\n",
    "    )\n",
    "    \n",
    "    random_key = random.PRNGKey(key_adjust + 2 * num_repeats + i)\n",
    "    nn_beta = initialize_DON(random_key)\n",
    "    nn_beta, losses_beta = train_DON(nn_beta, omega_beta, x, y_beta, max_iterations)\n",
    "\n",
    "    # Compute MSE for each omega_test\n",
    "    mse_normal_results = []\n",
    "    mse_uniform_results = []\n",
    "    mse_beta_results = []\n",
    "\n",
    "    for omega_test in omega_tests:\n",
    "        omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "        y_test = jnp.sin(omega_test_array * x_test)\n",
    "\n",
    "        mse_normal = np.mean(\n",
    "            (y_test - nn_normal.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "            ** 2\n",
    "        )\n",
    "        mse_uniform = np.mean(\n",
    "            (\n",
    "                y_test\n",
    "                - nn_uniform.predict(omega_test_array, x_test).reshape(y_test.shape)\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "        mse_beta = np.mean(\n",
    "            (y_test - nn_beta.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "            ** 2\n",
    "        )\n",
    "\n",
    "        mse_normal_results.append(mse_normal)\n",
    "        mse_uniform_results.append(mse_uniform)\n",
    "        mse_beta_results.append(mse_beta)\n",
    "\n",
    "    # Store results for this repetition\n",
    "    all_mse_normal_results.append(mse_normal_results)\n",
    "    all_mse_uniform_results.append(mse_uniform_results)\n",
    "    all_mse_beta_results.append(mse_beta_results)\n",
    "\n",
    "# Compute mean MSE across all repetitions for each omega\n",
    "mean_mse_normal = np.mean(all_mse_normal_results, axis=0)\n",
    "mean_mse_uniform = np.mean(all_mse_uniform_results, axis=0)\n",
    "mean_mse_beta = np.mean(all_mse_beta_results, axis=0)\n",
    "\n",
    "# Plot mean MSE vs Omega\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\"]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot mean MSE results (log scale)\n",
    "ax.semilogy(\n",
    "    omega_tests,\n",
    "    mean_mse_normal,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    "    label=\"Normal\",\n",
    "    color=colors[0],\n",
    ")\n",
    "ax.semilogy(\n",
    "    omega_tests,\n",
    "    mean_mse_uniform,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    "    label=\"Uniform\",\n",
    "    color=colors[1],\n",
    ")\n",
    "ax.semilogy(\n",
    "    omega_tests,\n",
    "    mean_mse_beta,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    "    label=\"Beta\",\n",
    "    color=colors[2],\n",
    ")\n",
    "\n",
    "# Add vertical dotted lines at boundaries\n",
    "ax.axvline(omega_min, color=\"k\", linestyle=\":\", alpha=0.7, linewidth=1.5)\n",
    "ax.axvline(omega_max, color=\"k\", linestyle=\":\", alpha=0.7, linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax.set_ylabel(\"Mean MSE (log scale)\", fontsize=12)\n",
    "ax.set_title(\"Mean Prediction Error (Log Scale)\", fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6, which=\"both\")\n",
    "\n",
    "# Add minor grid lines for the log scale\n",
    "ax.minorticks_on()\n",
    "ax.grid(which=\"minor\", linestyle=\":\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
