{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random, vmap, jit, grad, value_and_grad\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from functools import partial\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, layer_sizes, key, activation_fn=jax.nn.tanh):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.params = self.initialize_params(layer_sizes, key)\n",
    "\n",
    "    def initialize_params(self, layer_sizes, key):\n",
    "        params = []\n",
    "        keys = random.split(key, len(layer_sizes) - 1)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W_key, b_key = random.split(keys[i])\n",
    "            # Xavier initialization for weights\n",
    "            in_dim = layer_sizes[i]\n",
    "            out_dim = layer_sizes[i+1]\n",
    "            W = random.uniform(W_key, (in_dim, out_dim), minval=-jnp.sqrt(6 / (in_dim + out_dim)), maxval=jnp.sqrt(6 / (in_dim + out_dim)))\n",
    "            # Initialize biases with zeros\n",
    "            b = jnp.zeros(out_dim)\n",
    "            params.append((W, b))\n",
    "        return params\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward(self, params, x):\n",
    "        for W, b in params[:-1]:\n",
    "            # Linear transformation\n",
    "            x = jnp.dot(x, W) + b\n",
    "            # Apply activation function\n",
    "            x = self.activation_fn(x)\n",
    "        # Output layer (no activation function)\n",
    "        W, b = params[-1]\n",
    "        x = jnp.dot(x, W) + b\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predict output for input x\n",
    "        return vmap(self.forward, in_axes=(None, 0))(self.params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepONet:\n",
    "    def __init__(self, branch_layer_sizes, trunk_layer_sizes, key, activation_fn=jax.nn.tanh):\n",
    "        # Initialize branch and trunk networks\n",
    "        branch_key, trunk_key = random.split(key)\n",
    "        self.branch_net = FeedForwardNN(branch_layer_sizes, branch_key, activation_fn)\n",
    "        self.trunk_net = FeedForwardNN(trunk_layer_sizes, trunk_key, activation_fn)\n",
    "\n",
    "    def params(self):\n",
    "        return [self.branch_net.params, self.trunk_net.params]\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward(self, params, branch_input, trunk_input):\n",
    "        # Forward pass through branch and trunk networks\n",
    "        branch_output = self.branch_net.forward(params[0], branch_input)\n",
    "        trunk_output = self.trunk_net.forward(params[1], trunk_input)\n",
    "        # Combine outputs using inner product\n",
    "        return jnp.dot(branch_output, trunk_output.T)\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward_squeeze(self, params, branch_input, trunk_input):\n",
    "        return self.forward(params, branch_input, trunk_input).squeeze()\n",
    "\n",
    "    def predict(self, branch_input, trunk_input):\n",
    "        # Predict output for given inputs\n",
    "        return vmap(self.forward, in_axes=(None, 0, 0))(self.params(), branch_input, trunk_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Parameter omega\n",
    "def generate_omega(omega_min, omega_max, n_samples, key):\n",
    "    \n",
    "    # # Normally distributed omega\n",
    "\n",
    "    # Parameters for the true normal distribution\n",
    "    mean = (omega_max + omega_min) / 2\n",
    "    std = 2  # Smaller std = tighter around center\n",
    "\n",
    "    # Rejection sampling: draw until we have enough inside bounds\n",
    "    samples = []\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        # Use JAX's random.normal instead of NumPy's\n",
    "        proposed = random.normal(key, shape=(n_samples,)) * std + mean\n",
    "        proposed = np.array(proposed)  # Convert to NumPy for boolean masking\n",
    "        accepted = proposed[(proposed >= omega_min) & (proposed <= omega_max)]\n",
    "        samples.extend(accepted.tolist())\n",
    "        key, _ = random.split(key)  # Update key for next batch\n",
    "\n",
    "    omega_normal = np.array(samples[:n_samples])\n",
    "\n",
    "    # # Uniformly distributed omega\n",
    "\n",
    "    omega_uniform = random.uniform(\n",
    "        key, shape=(n_samples,), minval=omega_min, maxval=omega_max\n",
    "    )\n",
    "\n",
    "    # # Beta distributed omega\n",
    "\n",
    "    # Parameters for the beta distribution\n",
    "    alpha = 0.4\n",
    "    beta = 0.4\n",
    "\n",
    "    # Generate beta-distributed samples in [0, 1]\n",
    "    beta_samples = random.beta(key, alpha, beta, shape=(n_samples,))\n",
    "\n",
    "    # Scale the beta samples to the desired range [omega_min, omega_max]\n",
    "    omega_beta = omega_min + (omega_max - omega_min) * beta_samples\n",
    "\n",
    "    return omega_normal, omega_uniform, omega_beta\n",
    "\n",
    "omega_min = -1.0\n",
    "omega_max = 3 * np.pi\n",
    "\n",
    "random_key = random.PRNGKey(42)  # Random key for JAX\n",
    "n_samples = 1000\n",
    "omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "\n",
    "# --- Plot all three histograms in one figure ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histograms with histtype='step' (outline only)\n",
    "plt.hist(omega_normal, bins=50, density=True, histtype='step', linewidth=2, label='Normal')\n",
    "plt.hist(omega_uniform, bins=50, density=True, histtype='step', linewidth=2, label='Uniform')\n",
    "plt.hist(omega_beta, bins=50, density=True, histtype='step', linewidth=2, label='Beta(0.5, 0.5)')\n",
    "\n",
    "plt.title('Comparison of Omega Distributions', fontsize=14)\n",
    "plt.xlabel('Omega', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_DON(n_input_sensor):\n",
    "    # Create a neural network\n",
    "    key = random.PRNGKey(0)\n",
    "    branch_layer_sizes = [n_input_sensor, 50, 50, 10]\n",
    "    trunk_layer_sizes = [1, 50, 50, 10]\n",
    "    nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, key)\n",
    "\n",
    "    return nn\n",
    "\n",
    "def train_DON(nn,u,omega,x,y,max_iter):\n",
    "\n",
    "    def loss_fn(params, u, omega, x, y):\n",
    "        # return jnp.mean(jnp.mean(squared_errors_batch(params, u, x, y)))\n",
    "        return jnp.mean(squared_residual_batch(params, u, omega, x)) + jnp.mean(squared_errors_batch(params, u, x, y))\n",
    "\n",
    "    # Define training step with Adam optimizer\n",
    "    @jit\n",
    "    def train_step(params, opt_state, u, omega, x, y):    \n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, u, omega, x, y)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, opt_state, loss\n",
    "\n",
    "    # Define the loss function\n",
    "    u_x = jit(grad(nn.forward_squeeze, argnums=2))\n",
    "    squared_residual = jit(lambda params, u, omega, x: (u_x(params, u, x) - omega * jnp.cos(omega * x)) ** 2)\n",
    "    squared_residual_batch = jit(vmap(squared_residual, in_axes=(None, 0, 0, 0))) \n",
    "\n",
    "    # With or without data\n",
    "    # squared_error = jit(lambda params, u: nn.forward(params, u, 0) ** 2)\n",
    "    squared_error = jit(lambda params, u, x, y: (nn.forward(params, u, x) - y) ** 2)\n",
    "    squared_errors_batch = jit(vmap(squared_error, in_axes=(None, 0, 0, 0)))\n",
    "    \n",
    "    # Initialize the Adam optimizer with the learning rate schedule\n",
    "    learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "        init_value=0.01,\n",
    "        boundaries_and_scales={300: 0.2, 2000: 0.5}\n",
    "    )\n",
    "    optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "    opt_state = optimizer.init(nn.params())\n",
    "    losses = []\n",
    "\n",
    "    # Training loop\n",
    "    max_iterations = max_iter\n",
    "\n",
    "    # Training loop with tqdm progress bar\n",
    "    pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "    for epoch in pbar:\n",
    "        don_params, opt_state, current_loss = train_step(nn.params(), opt_state, u, omega, x, y)\n",
    "        nn.branch_net.params = don_params[0] \n",
    "        nn.trunk_net.params = don_params[1]\n",
    "        losses.append(current_loss)\n",
    "        if epoch % 100 == 0:\n",
    "            pbar.set_postfix({'loss': current_loss})\n",
    "        if current_loss < 1.0e-5:\n",
    "            break\n",
    "    \n",
    "    return nn, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = omega_beta\n",
    "\n",
    "# Generate random x values between -1 and 1 with a different random key\n",
    "key_x = random.PRNGKey(4)\n",
    "x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "# Compute y values for the generated omega and x\n",
    "y = jnp.sin(omega * x)\n",
    "\n",
    "n_input_sensor = 2\n",
    "# key_input_sensor = random.PRNGKey(5)\n",
    "# x_input_sensor = random.uniform(key_input_sensor, shape=(n_input_sensor,), minval=-1, maxval=1)\n",
    "\n",
    "# The points should not be symmetrically distributed when having very few sensor points\n",
    "x_sensor_min = -0.7\n",
    "x_sensor_max = 0.9\n",
    "x_input_sensor = np.linspace(x_sensor_min,x_sensor_max,n_input_sensor)\n",
    "\n",
    "u = jnp.cos(omega[:,None] * x_input_sensor) * omega[:,None]\n",
    "\n",
    "max_iterations = 5000\n",
    "\n",
    "nn = initialize_DON(n_input_sensor)\n",
    "\n",
    "nn, losses = train_DON(\n",
    "    nn, u, omega, x, y, max_iterations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_u = 50  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "\n",
    "# key_test = random.PRNGKey(10)\n",
    "# omega_tests = random.uniform(key_test, shape=(n_samples,), minval=omega_min-delta, maxval=omega_max+delta)\n",
    "omega_tests = jnp.linspace(omega_min - delta, omega_max + delta, n_test_u)\n",
    "\n",
    "u_tests = jnp.cos(omega_tests[:,None] * x_input_sensor) * omega_tests[:,None]\n",
    "\n",
    "mse_results = []\n",
    "\n",
    "for i in range(n_test_u):\n",
    "    omega_test_array = jnp.ones_like(x_test) * omega_tests[i]\n",
    "    y_test = jnp.sin(omega_test_array * x_test)\n",
    "    u_test = jnp.tile(u_tests[i], (len(x_test), 1))\n",
    "\n",
    "    mse = np.mean(\n",
    "        (y_test - nn.predict(u_test, x_test).reshape(y_test.shape))\n",
    "        ** 2\n",
    "    )\n",
    "    \n",
    "    mse_results.append(mse)\n",
    "    \n",
    "\n",
    "# Plot MSE vs Omega (optional)\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- First subplot: Omega distributions ---\n",
    "ax1.hist(omega, bins=50, density=True, histtype=\"step\", \n",
    "         linewidth=2, color=colors[0])\n",
    "\n",
    "ax1.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax1.set_ylabel(\"Density\", fontsize=12)\n",
    "ax1.set_title(\"Omega Distribution\", fontsize=14)\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# --- Second subplot: MSE results (log scale) ---\n",
    "ax2.semilogy(omega_tests, mse_results, \"o-\", linewidth=2, \n",
    "             markersize=5, color=colors[0])\n",
    "\n",
    "# Add vertical dotted lines at boundaries\n",
    "ax2.axvline(omega_min, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "ax2.axvline(omega_max, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax2.set_ylabel(\"MSE (log scale)\", fontsize=12)\n",
    "ax2.set_title(\"Prediction Error (Log Scale)\", fontsize=14)\n",
    "ax2.grid(True, linestyle=\"--\", alpha=0.6, which='both')\n",
    "\n",
    "# Add minor grid lines for the log scale\n",
    "ax2.minorticks_on()\n",
    "ax2.grid(which='minor', linestyle=':', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the results for nn\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot the loss over the number of iterations for nn\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].plot(range(len(losses)), losses, label=\"Loss\")\n",
    "axs[0].set_xlabel(\"Iteration\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the final results for nn\n",
    "omega_tests = jnp.linspace(omega_min, omega_max, 5)\n",
    "u_tests = jnp.cos(omega_tests[:,None] * x_input_sensor) * omega_tests[:,None]\n",
    "\n",
    "# omega_tests = omega_tests.at[1].set(0)\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(10, 6))  # Adjust layout if needed\n",
    "\n",
    "for omega_test, color, i in zip(omega_tests, colors, range(5)):\n",
    "    omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "    y_test = jnp.sin(omega_test_array * x_test)\n",
    "    u_test = jnp.tile(u_tests[i], (len(x_test), 1))\n",
    "\n",
    "    # Plot true sine function (dashed line)\n",
    "    axs[1].plot(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Sine function (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    # Plot neural network prediction (solid line)\n",
    "    axs[1].plot(\n",
    "        x_test,\n",
    "        nn.predict(u_test, x_test).reshape(y_test.shape),\n",
    "        color=color,\n",
    "        label=f\"Neural network (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "# Add vertical dotted lines at each sensor point\n",
    "for x_sensor in x_input_sensor:\n",
    "    axs[1].axvline(x=x_sensor, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "# Add legend outside the plot\n",
    "axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Set titles for each subplot\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[1].set_title(\"Solutions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_adjust = 42\n",
    "\n",
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_u = 50  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "omega_tests = jnp.linspace(omega_min - delta, omega_max + delta, n_test_u)\n",
    "\n",
    "# Number of repetitions for training\n",
    "n_input_sensor_list = [1, 2, 4, 8]\n",
    "max_iterations = 4000\n",
    "\n",
    "num_repeats = 5\n",
    "\n",
    "mean_mse_results = []\n",
    "\n",
    "for i in range(len(n_input_sensor_list)): \n",
    "    print(f\"Training repetition {i + 1}/{len(n_input_sensor_list)}\")\n",
    "\n",
    "    # List to store MSE results for each repeat\n",
    "    all_mse_results = []\n",
    "\n",
    "    for j in range(num_repeats):\n",
    "        print(f\"Repetition {j + 1}/{num_repeats}\")\n",
    "\n",
    "        random_key = random.PRNGKey(key_adjust + j)  # Random key for JAX\n",
    "        omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "\n",
    "        omega = omega_beta\n",
    "\n",
    "        # Generate random x values between -1 and 1 with a different random key\n",
    "        key_x = random.PRNGKey(key_adjust + j)\n",
    "        x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "        # Compute y values for the generated omega and x\n",
    "        y = jnp.sin(omega * x)\n",
    "\n",
    "        n_input_sensor = n_input_sensor_list[i]\n",
    "        x_input_sensor = np.linspace(x_sensor_min,x_sensor_max,n_input_sensor)\n",
    "\n",
    "        u = jnp.cos(omega[:, None] * x_input_sensor) * omega[:, None]\n",
    "\n",
    "        u_tests = jnp.cos(omega_tests[:, None] * x_input_sensor) * omega_tests[:, None]\n",
    "\n",
    "        nn = initialize_DON(n_input_sensor)\n",
    "\n",
    "        nn, losses = train_DON(\n",
    "            nn, u, omega, x, y, max_iterations\n",
    "        )\n",
    "\n",
    "        mse_results = []\n",
    "\n",
    "        for j in range(n_test_u):\n",
    "            omega_test_array = jnp.ones_like(x_test) * omega_tests[j]\n",
    "            y_test = jnp.sin(omega_test_array * x_test)\n",
    "            u_test = jnp.tile(u_tests[j], (len(x_test), 1))\n",
    "\n",
    "            mse = np.mean(\n",
    "                (y_test - nn.predict(u_test, x_test).reshape(y_test.shape))\n",
    "                ** 2\n",
    "            )\n",
    "            \n",
    "            mse_results.append(mse)\n",
    "\n",
    "        # Save the MSE results for this repeat\n",
    "        all_mse_results.append(mse_results)\n",
    "\n",
    "    mean_mse_results.append(np.mean(all_mse_results, axis=0))\n",
    "\n",
    "# Plot all MSE results together\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, mean_mse_result in enumerate(mean_mse_results):\n",
    "    plt.semilogy(\n",
    "        omega_tests,\n",
    "        mean_mse_result,\n",
    "        label=f\"Input sensor points={n_input_sensor_list[i]}\",\n",
    "        color=colors[i % len(colors)],\n",
    "        marker=\"o\",\n",
    "        linewidth=2,\n",
    "        markersize=5,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Omega\", fontsize=12)\n",
    "plt.ylabel(\"MSE (log scale)\", fontsize=12)\n",
    "plt.title(\"MSE results for different numbers of sensor points\", fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6, which=\"both\")\n",
    "\n",
    "# Add vertical dotted lines at boundaries\n",
    "plt.axvline(omega_min, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "plt.axvline(omega_max, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
