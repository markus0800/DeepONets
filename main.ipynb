{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main notebook for project in *Special topics in computational science and engineering*\n",
    "\n",
    "Authors: Laurits Fog Balstrup, Markus Sandnes\n",
    "\n",
<<<<<<< HEAD
    "Notebook goals:\n",
    "- Train DON for one dimensional time dependent convection diffusion equation\n",
    "- Investigate if Neural operators (DeepONets FNO) can solve the one dimensional time depoendet convection diffusion euqation for a family of forcing terms \n",
    "\n"
=======
    "The original results where obtained from other notebooks, that have simply been copied into one here. Many of the experiments where independent of each other, hence the choice of performing them in seperate notebooks originally. "
>>>>>>> 17878a4e3d0c5b4368e7f682c0222a4d31d16243
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and solver for differential equations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fem_1d_for_students_kopi import *\n",
    "\n",
    "import jax\n",
    "from jax import random\n",
    "from scipy.sparse import csc_matrix, csc_array\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import scipy\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from jax import random, vmap, jit, grad, value_and_grad\n",
    "import optax\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, layer_sizes, key, activation_fn=jax.nn.tanh):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_fn = activation_fn\n",
    "        self.params = self.initialize_params(layer_sizes, key)\n",
    "\n",
    "    def initialize_params(self, layer_sizes, key):\n",
    "        params = []\n",
    "        keys = random.split(key, len(layer_sizes) - 1)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            W_key, b_key = random.split(keys[i])\n",
    "            # Xavier initialization for weights\n",
    "            in_dim = layer_sizes[i]\n",
    "            out_dim = layer_sizes[i+1]\n",
    "            W = random.uniform(W_key, (in_dim, out_dim), minval=-jnp.sqrt(6 / (in_dim + out_dim)), maxval=jnp.sqrt(6 / (in_dim + out_dim)))\n",
    "            # Initialize biases with zeros\n",
    "            b = jnp.zeros(out_dim)\n",
    "            params.append((W, b))\n",
    "        return params\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward(self, params, x):\n",
    "        for W, b in params[:-1]:\n",
    "            # Linear transformation\n",
    "            x = jnp.dot(x, W) + b\n",
    "            # Apply activation function\n",
    "            x = self.activation_fn(x)\n",
    "        # Output layer (no activation function)\n",
    "        W, b = params[-1]\n",
    "        x = jnp.dot(x, W) + b\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predict output for input x\n",
    "        return vmap(self.forward, in_axes=(None, 0))(self.params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepONet:\n",
    "    def __init__(self, branch_layer_sizes, trunk_layer_sizes, key, activation_fn=jax.nn.tanh):\n",
    "        # Initialize branch and trunk networks\n",
    "        branch_key, trunk_key = random.split(key)\n",
    "        self.branch_net = FeedForwardNN(branch_layer_sizes, branch_key, activation_fn)\n",
    "        self.trunk_net = FeedForwardNN(trunk_layer_sizes, trunk_key, activation_fn)\n",
    "\n",
    "    def params(self):\n",
    "        return [self.branch_net.params, self.trunk_net.params]\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward(self, params, branch_input, trunk_input):\n",
    "        # Forward pass through branch and trunk networks\n",
    "        branch_output = self.branch_net.forward(params[0], branch_input)\n",
    "        trunk_output = self.trunk_net.forward(params[1], trunk_input)\n",
    "        # Combine outputs using inner product\n",
    "        return jnp.dot(branch_output, trunk_output.T)\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward_squeeze(self, params, branch_input, trunk_input):\n",
    "        return self.forward(params, branch_input, trunk_input).squeeze()\n",
    "\n",
    "    def predict(self, branch_input, trunk_input):\n",
    "        # Predict output for given inputs\n",
    "        return vmap(self.forward, in_axes=(None, 0, 0))(self.params(), branch_input, trunk_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter distribution 1D case\n",
    "\n",
    "Define\n",
    "\n",
    "\\begin{align}\n",
    "    \\Omega&:=\\{f(x)=\\omega\\cos(\\omega x),\\quad \\omega\\in[a,b]\\}, \\\\\n",
    "    \\Omega_{test}&:=\\{f(x)=\\omega\\cos(\\omega x),\\quad \\omega\\in[a-\\delta,b+\\delta]\\}.\n",
    "\\end{align}\n",
    "\n",
    "We test the accuracy of the DON on the test input function space $\\Omega_{test}$ for different sampling distributions:\n",
    "\n",
    "Uniform sampling of $\\omega$ in $[a,b]$. \n",
    "\n",
    "Normally distributed sampling restricted to $\\omega$ in $[a,b]$ by cutting off tails. \n",
    "\n",
    "Beta-distribution of $\\omega$ in $[a,b]$ with $\\alpha=\\beta=0.4$.\n",
    "\n",
    "We compute the MSE for a number of input functions $f\\in\\Omega_{test}$ where $a=-1,b=3\\pi$ and $\\delta=1$. To reduce the noise in the results, we train and test 5 times and take to mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameter omega\n",
    "def generate_omega(omega_min, omega_max, n_samples, key):\n",
    "    \n",
    "    # # Normally distributed omega\n",
    "\n",
    "    # Parameters for the true normal distribution\n",
    "    mean = (omega_max + omega_min) / 2\n",
    "    std = 2  # Smaller std = tighter around center\n",
    "\n",
    "    # Rejection sampling: draw until we have enough inside bounds\n",
    "    samples = []\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        # Use JAX's random.normal instead of NumPy's\n",
    "        proposed = random.normal(key, shape=(n_samples,)) * std + mean\n",
    "        proposed = np.array(proposed)  # Convert to NumPy for boolean masking\n",
    "        accepted = proposed[(proposed >= omega_min) & (proposed <= omega_max)]\n",
    "        samples.extend(accepted.tolist())\n",
    "        key, _ = random.split(key)  # Update key for next batch\n",
    "\n",
    "    omega_normal = np.array(samples[:n_samples])\n",
    "\n",
    "    # # Uniformly distributed omega\n",
    "\n",
    "    omega_uniform = random.uniform(\n",
    "        key, shape=(n_samples,), minval=omega_min, maxval=omega_max\n",
    "    )\n",
    "\n",
    "    # # Beta distributed omega\n",
    "\n",
    "    # Parameters for the beta distribution\n",
    "    alpha = 0.4\n",
    "    beta = 0.4\n",
    "\n",
    "    # Generate beta-distributed samples in [0, 1]\n",
    "    beta_samples = random.beta(key, alpha, beta, shape=(n_samples,))\n",
    "\n",
    "    # Scale the beta samples to the desired range [omega_min, omega_max]\n",
    "    omega_beta = omega_min + (omega_max - omega_min) * beta_samples\n",
    "\n",
    "    return omega_normal, omega_uniform, omega_beta\n",
    "\n",
    "omega_min = -1.0\n",
    "omega_max = 3 * np.pi\n",
    "\n",
    "random_key = random.PRNGKey(42)  # Random key for JAX\n",
    "n_samples = 5000\n",
    "omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "\n",
    "# --- Plot all three histograms in one figure ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot histograms with histtype='step' (outline only)\n",
    "plt.hist(\n",
    "    omega_normal, bins=50, density=True, histtype=\"step\", linewidth=2, label=\"Normal\"\n",
    ")\n",
    "plt.hist(\n",
    "    omega_uniform, bins=50, density=True, histtype=\"step\", linewidth=2, label=\"Uniform\"\n",
    ")\n",
    "plt.hist(\n",
    "    omega_beta,\n",
    "    bins=50,\n",
    "    density=True,\n",
    "    histtype=\"step\",\n",
    "    linewidth=2,\n",
    "    label=\"Beta(0.4, 0.4)\",\n",
    ")\n",
    "\n",
    "plt.title(\"Comparison of Omega Distributions\", fontsize=14)\n",
    "plt.xlabel(\"Omega\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate random x values between -1 and 1 with a different random key\n",
    "key_x = random.PRNGKey(2)\n",
    "x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "# Compute y values for the generated omega and x\n",
    "y_normal = jnp.sin(omega_normal * x)\n",
    "\n",
    "y_uniform = jnp.sin(omega_uniform * x)\n",
    "\n",
    "y_beta = jnp.sin(omega_beta * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_DON(key=None):\n",
    "    # Use the provided key or default to random.PRNGKey(0)\n",
    "    if key is None:\n",
    "        key = random.PRNGKey(0)\n",
    "    \n",
    "    # Create a neural network\n",
    "    branch_layer_sizes = [1, 50, 50, 10]\n",
    "    trunk_layer_sizes = [1, 50, 50, 10]\n",
    "    nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, key)\n",
    "\n",
    "    return nn\n",
    "\n",
    "def train_DON(nn, omega, x, y, max_iter, batch_sampling=False, batch_ratio=0.2):\n",
    "    def loss_fn(params, omega, x, y):\n",
    "        return jnp.mean(squared_residual_batch(params, omega, x)) + jnp.mean(\n",
    "            squared_errors_batch(params, omega, x, y)\n",
    "        )\n",
    "\n",
    "    # Define training step with Adam optimizer\n",
    "    @jit\n",
    "    def train_step(params, opt_state, omega, x, y):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, omega, x, y)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, opt_state, loss\n",
    "\n",
    "    # Define the loss function\n",
    "    u_x = jit(grad(nn.forward_squeeze, argnums=2))\n",
    "    squared_residual = jit(\n",
    "        lambda params, omega, x: (u_x(params, omega, x) - omega * jnp.cos(omega * x))\n",
    "        ** 2\n",
    "    )\n",
    "    squared_residual_batch = jit(vmap(squared_residual, in_axes=(None, 0, 0)))\n",
    "\n",
    "    # With or without data\n",
    "    # squared_error = jit(lambda params, omega: nn.forward(params, omega, 0) ** 2)\n",
    "    squared_error = jit(\n",
    "        lambda params, omega, x, y: (nn.forward(params, omega, x) - y) ** 2\n",
    "    )\n",
    "    squared_errors_batch = jit(vmap(squared_error, in_axes=(None, 0, 0, 0)))\n",
    "\n",
    "    # Training loop\n",
    "    max_iterations = max_iter\n",
    "\n",
    "    # Training loop with tqdm progress bar with batch sampling\n",
    "    if batch_sampling:\n",
    "        # Initialize the Adam optimizer with the learning rate schedule\n",
    "        learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "            init_value=0.01,\n",
    "            boundaries_and_scales={300: 0.2, 2000: 0.5, 3000: 0.1, 4000: 0.1}\n",
    "            # boundaries_and_scales={300: 0.2, 2000: 0.5},\n",
    "        )\n",
    "        optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "        opt_state = optimizer.init(nn.params())\n",
    "        losses = []\n",
    "\n",
    "        pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "        for i, epoch in enumerate(pbar):\n",
    "            \n",
    "            idx = np.random.choice(len(omega),int(len(omega)*batch_ratio), replace = False)\n",
    "            \n",
    "            don_params, opt_state, current_loss = train_step(\n",
    "                nn.params(), opt_state, omega[idx], x[idx], y[idx]\n",
    "            )\n",
    "\n",
    "            nn.branch_net.params = don_params[0]\n",
    "            nn.trunk_net.params = don_params[1]\n",
    "            losses.append(current_loss)\n",
    "            if epoch % 100 == 0:\n",
    "                pbar.set_postfix({\"loss\": current_loss})\n",
    "            if current_loss < 1.0e-5:\n",
    "                break\n",
    "    else:\n",
    "        # Initialize the Adam optimizer with the learning rate schedule\n",
    "        learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "            init_value=0.01, boundaries_and_scales={300: 0.2, 2000: 0.5}\n",
    "        )\n",
    "        optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "        opt_state = optimizer.init(nn.params())\n",
    "        losses = []\n",
    "\n",
    "        # Training loop without batch sampling\n",
    "        pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "        for i, epoch in enumerate(pbar):\n",
    "            don_params, opt_state, current_loss = train_step(\n",
    "                nn.params(), opt_state, omega, x, y\n",
    "            )\n",
    "            nn.branch_net.params = don_params[0]\n",
    "            nn.trunk_net.params = don_params[1]\n",
    "            losses.append(current_loss)\n",
    "            if epoch % 100 == 0:\n",
    "                pbar.set_postfix({\"loss\": current_loss})\n",
    "            if current_loss < 1.0e-5:\n",
    "                break\n",
    "\n",
    "    return nn, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 5000\n",
    "\n",
    "nn_normal = initialize_DON()\n",
    "\n",
    "nn_normal, losses_normal = train_DON(\n",
    "    nn_normal, omega_normal, x, y_normal, max_iterations\n",
    ")\n",
    "\n",
    "nn_uniform = initialize_DON()\n",
    "\n",
    "nn_uniform, losses_uniform = train_DON(\n",
    "    nn_uniform, omega_uniform, x, y_uniform, max_iterations\n",
    ")\n",
    "\n",
    "nn_beta = initialize_DON()\n",
    "\n",
    "nn_beta, losses_beta = train_DON(nn_beta, omega_beta, x, y_beta, max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_omegas = 50  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "omega_tests = jnp.linspace(omega_min - delta, omega_max + delta, n_test_omegas)\n",
    "\n",
    "mse_normal_results = []\n",
    "mse_uniform_results = []\n",
    "mse_beta_results = []\n",
    "\n",
    "for omega_test in omega_tests:\n",
    "    omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "    y_test = jnp.sin(omega_test_array * x_test)\n",
    "\n",
    "    mse_normal = np.mean(\n",
    "        (y_test - nn_normal.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "        ** 2\n",
    "    )\n",
    "    mse_uniform = np.mean(\n",
    "        (y_test - nn_uniform.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "        ** 2\n",
    "    )\n",
    "    mse_beta = np.mean(\n",
    "        (y_test - nn_beta.predict(omega_test_array, x_test).reshape(y_test.shape)) ** 2\n",
    "    )\n",
    "\n",
    "    mse_normal_results.append(mse_normal)\n",
    "    mse_uniform_results.append(mse_uniform)\n",
    "    mse_beta_results.append(mse_beta)\n",
    "\n",
    "\n",
    "# Plot MSE vs Omega (optional)\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- First subplot: Omega distributions ---\n",
    "ax1.hist(omega_normal, bins=50, density=True, histtype=\"step\", \n",
    "         linewidth=2, label=\"Normal\", color=colors[0])\n",
    "ax1.hist(omega_uniform, bins=50, density=True, histtype=\"step\", \n",
    "         linewidth=2, label=\"Uniform\", color=colors[1])\n",
    "ax1.hist(omega_beta, bins=50, density=True, histtype=\"step\", \n",
    "         linewidth=2, label=\"Beta\", color=colors[2])\n",
    "\n",
    "ax1.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax1.set_ylabel(\"Density\", fontsize=12)\n",
    "ax1.set_title(\"Omega Distributions\", fontsize=14)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# --- Second subplot: MSE results (log scale) ---\n",
    "ax2.semilogy(omega_tests, mse_normal_results, \"o-\", linewidth=2, \n",
    "             markersize=5, label=\"Normal\", color=colors[0])\n",
    "ax2.semilogy(omega_tests, mse_uniform_results, \"o-\", linewidth=2, \n",
    "             markersize=5, label=\"Uniform\", color=colors[1])\n",
    "ax2.semilogy(omega_tests, mse_beta_results, \"o-\", linewidth=2, \n",
    "             markersize=5, label=\"Beta\", color=colors[2])\n",
    "\n",
    "# Add vertical dotted lines at boundaries\n",
    "ax2.axvline(omega_min, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "ax2.axvline(omega_max, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax2.set_ylabel(\"MSE (log scale)\", fontsize=12)\n",
    "ax2.set_title(\"Prediction Error (Log Scale)\", fontsize=14)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, linestyle=\"--\", alpha=0.6, which='both')\n",
    "\n",
    "# Add minor grid lines for the log scale\n",
    "ax2.minorticks_on()\n",
    "ax2.grid(which='minor', linestyle=':', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the results for nn\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(4, 1, figsize=(14, 20))\n",
    "\n",
    "# Plot the loss over the number of iterations for nn\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].plot(range(len(losses_normal)), losses_normal, label=\"Loss (normal)\")\n",
    "axs[0].plot(range(len(losses_uniform)), losses_uniform, label=\"Loss (uniform)\")\n",
    "axs[0].plot(range(len(losses_beta)), losses_beta, label=\"Loss (beta)\")\n",
    "axs[0].set_xlabel(\"Iteration\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the final results for nn\n",
    "omega_tests = jnp.linspace(omega_min, omega_max, 5)\n",
    "# key_uniform = random.PRNGKey(10)\n",
    "# omega_tests = random.uniform(key_uniform, shape=(5,), minval=omega_min, maxval=omega_max)\n",
    "omega_tests = omega_tests.at[1].set(0)\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(10, 6))  # Adjust layout if needed\n",
    "\n",
    "for omega_test, color in zip(omega_tests, colors):\n",
    "    omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "    y_test = jnp.sin(omega_test_array * x_test)\n",
    "    # Plot true sine function (dashed line)\n",
    "    axs[1].plot(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Sine function (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    # Plot neural network prediction (solid line)\n",
    "    axs[1].plot(\n",
    "        x_test,\n",
    "        nn_normal.predict(omega_test_array, x_test).reshape(y_test.shape),\n",
    "        color=color,\n",
    "        label=f\"Neural network (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    axs[2].plot(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Sine function (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    # Plot neural network prediction (solid line)\n",
    "    axs[2].plot(\n",
    "        x_test,\n",
    "        nn_uniform.predict(omega_test_array, x_test).reshape(y_test.shape),\n",
    "        color=color,\n",
    "        label=f\"Neural network (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    axs[3].plot(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Sine function (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    # Plot neural network prediction (solid line)\n",
    "    axs[3].plot(\n",
    "        x_test,\n",
    "        nn_beta.predict(omega_test_array, x_test).reshape(y_test.shape),\n",
    "        color=color,\n",
    "        label=f\"Neural network (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "# Add legend outside the plot\n",
    "axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "axs[2].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "axs[3].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Set titles for each subplot\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[1].set_title(\"Normal\")\n",
    "axs[2].set_title(\"Uniform\") \n",
    "axs[3].set_title(\"Beta\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_omegas = 50  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "omega_tests = jnp.linspace(omega_min - delta, omega_max + delta, n_test_omegas)\n",
    "\n",
    "# Number of repetitions for training\n",
    "num_repeats = 5\n",
    "\n",
    "# Initialize lists to store MSE results for each repetition\n",
    "all_mse_normal_results = []\n",
    "all_mse_uniform_results = []\n",
    "all_mse_beta_results = []\n",
    "\n",
    "n_samples = 500\n",
    "max_iterations = 4000\n",
    "\n",
    "key_adjust = 42\n",
    "\n",
    "for i in range(num_repeats):\n",
    "    print(f\"Training repetition {i + 1}/{num_repeats}\")\n",
    "\n",
    "    random_key = random.PRNGKey(key_adjust + i)  # Random key for JAX\n",
    "    omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "\n",
    "    # Generate random x values between -1 and 1 with a different random key\n",
    "    key_x = random.PRNGKey(key_adjust + i)\n",
    "    x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "    # Compute y values for the generated omega and x\n",
    "    y_normal = jnp.sin(omega_normal * x)\n",
    "\n",
    "    y_uniform = jnp.sin(omega_uniform * x)\n",
    "\n",
    "    y_beta = jnp.sin(omega_beta * x)\n",
    "\n",
    "    # Train DON models\n",
    "    random_key = random.PRNGKey(i)\n",
    "    nn_normal = initialize_DON(key_adjust + random_key)\n",
    "    nn_normal, losses_normal = train_DON(\n",
    "        nn_normal, omega_normal, x, y_normal, max_iterations\n",
    "    )\n",
    "\n",
    "    random_key = random.PRNGKey(key_adjust + num_repeats + i)\n",
    "    nn_uniform = initialize_DON(random_key)\n",
    "    nn_uniform, losses_uniform = train_DON(\n",
    "        nn_uniform, omega_uniform, x, y_uniform, max_iterations\n",
    "    )\n",
    "    \n",
    "    random_key = random.PRNGKey(key_adjust + 2 * num_repeats + i)\n",
    "    nn_beta = initialize_DON(random_key)\n",
    "    nn_beta, losses_beta = train_DON(nn_beta, omega_beta, x, y_beta, max_iterations)\n",
    "\n",
    "    # Compute MSE for each omega_test\n",
    "    mse_normal_results = []\n",
    "    mse_uniform_results = []\n",
    "    mse_beta_results = []\n",
    "\n",
    "    for omega_test in omega_tests:\n",
    "        omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "        y_test = jnp.sin(omega_test_array * x_test)\n",
    "\n",
    "        mse_normal = np.mean(\n",
    "            (y_test - nn_normal.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "            ** 2\n",
    "        )\n",
    "        mse_uniform = np.mean(\n",
    "            (\n",
    "                y_test\n",
    "                - nn_uniform.predict(omega_test_array, x_test).reshape(y_test.shape)\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "        mse_beta = np.mean(\n",
    "            (y_test - nn_beta.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "            ** 2\n",
    "        )\n",
    "\n",
    "        mse_normal_results.append(mse_normal)\n",
    "        mse_uniform_results.append(mse_uniform)\n",
    "        mse_beta_results.append(mse_beta)\n",
    "\n",
    "    # Store results for this repetition\n",
    "    all_mse_normal_results.append(mse_normal_results)\n",
    "    all_mse_uniform_results.append(mse_uniform_results)\n",
    "    all_mse_beta_results.append(mse_beta_results)\n",
    "\n",
    "# Compute mean MSE across all repetitions for each omega\n",
    "mean_mse_normal = np.mean(all_mse_normal_results, axis=0)\n",
    "mean_mse_uniform = np.mean(all_mse_uniform_results, axis=0)\n",
    "mean_mse_beta = np.mean(all_mse_beta_results, axis=0)\n",
    "\n",
    "# Plot mean MSE vs Omega\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\"]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot mean MSE results (log scale)\n",
    "ax.semilogy(\n",
    "    omega_tests,\n",
    "    mean_mse_normal,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    "    label=\"Normal\",\n",
    "    color=colors[0],\n",
    ")\n",
    "ax.semilogy(\n",
    "    omega_tests,\n",
    "    mean_mse_uniform,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    "    label=\"Uniform\",\n",
    "    color=colors[1],\n",
    ")\n",
    "ax.semilogy(\n",
    "    omega_tests,\n",
    "    mean_mse_beta,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    "    label=\"Beta\",\n",
    "    color=colors[2],\n",
    ")\n",
    "\n",
    "# Add vertical dotted lines at boundaries\n",
    "ax.axvline(omega_min, color=\"k\", linestyle=\":\", alpha=0.7, linewidth=1.5)\n",
    "ax.axvline(omega_max, color=\"k\", linestyle=\":\", alpha=0.7, linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax.set_ylabel(\"Mean MSE (log scale)\", fontsize=12)\n",
    "ax.set_title(\"Mean Prediction Error (Log Scale)\", fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6, which=\"both\")\n",
    "\n",
    "# Add minor grid lines for the log scale\n",
    "ax.minorticks_on()\n",
    "ax.grid(which=\"minor\", linestyle=\":\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of input sensors 1D case\n",
    "\n",
    "In this case, the input function is completely determined by a single variable, namely $\\omega$. But now we input the input function $f$ to the branch net in the form of sensor point values, i.e. $(f(x_1),f(x_2),...,f(x_m))$. We once again train the DON's on input functions $f\\in\\Omega$ where $a=-1,b=3\\pi$ and test on $f\\in\\Omega_{test}$ where $\\delta=1$. The training and testing is performed 5 times to estimate the mean MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Parameter omega\n",
    "def generate_omega(omega_min, omega_max, n_samples, key):\n",
    "    \n",
    "    # # Normally distributed omega\n",
    "\n",
    "    # Parameters for the true normal distribution\n",
    "    mean = (omega_max + omega_min) / 2\n",
    "    std = 2  # Smaller std = tighter around center\n",
    "\n",
    "    # Rejection sampling: draw until we have enough inside bounds\n",
    "    samples = []\n",
    "\n",
    "    while len(samples) < n_samples:\n",
    "        # Use JAX's random.normal instead of NumPy's\n",
    "        proposed = random.normal(key, shape=(n_samples,)) * std + mean\n",
    "        proposed = np.array(proposed)  # Convert to NumPy for boolean masking\n",
    "        accepted = proposed[(proposed >= omega_min) & (proposed <= omega_max)]\n",
    "        samples.extend(accepted.tolist())\n",
    "        key, _ = random.split(key)  # Update key for next batch\n",
    "\n",
    "    omega_normal = np.array(samples[:n_samples])\n",
    "\n",
    "    # # Uniformly distributed omega\n",
    "\n",
    "    omega_uniform = random.uniform(\n",
    "        key, shape=(n_samples,), minval=omega_min, maxval=omega_max\n",
    "    )\n",
    "\n",
    "    # # Beta distributed omega\n",
    "\n",
    "    # Parameters for the beta distribution\n",
    "    alpha = 0.4\n",
    "    beta = 0.4\n",
    "\n",
    "    # Generate beta-distributed samples in [0, 1]\n",
    "    beta_samples = random.beta(key, alpha, beta, shape=(n_samples,))\n",
    "\n",
    "    # Scale the beta samples to the desired range [omega_min, omega_max]\n",
    "    omega_beta = omega_min + (omega_max - omega_min) * beta_samples\n",
    "\n",
    "    return omega_normal, omega_uniform, omega_beta\n",
    "\n",
    "omega_min = -1.0\n",
    "omega_max = 3 * np.pi\n",
    "\n",
    "random_key = random.PRNGKey(42)  # Random key for JAX\n",
    "n_samples = 1000\n",
    "omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_DON(n_input_sensor):\n",
    "    # Create a neural network\n",
    "    key = random.PRNGKey(0)\n",
    "    branch_layer_sizes = [n_input_sensor, 50, 50, 10]\n",
    "    trunk_layer_sizes = [1, 50, 50, 10]\n",
    "    nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, key)\n",
    "\n",
    "    return nn\n",
    "\n",
    "def train_DON(nn,u,omega,x,y,max_iter):\n",
    "\n",
    "    def loss_fn(params, u, omega, x, y):\n",
    "        # return jnp.mean(jnp.mean(squared_errors_batch(params, u, x, y)))\n",
    "        return jnp.mean(squared_residual_batch(params, u, omega, x)) + jnp.mean(squared_errors_batch(params, u, x, y))\n",
    "\n",
    "    # Define training step with Adam optimizer\n",
    "    @jit\n",
    "    def train_step(params, opt_state, u, omega, x, y):    \n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, u, omega, x, y)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, opt_state, loss\n",
    "\n",
    "    # Define the loss function\n",
    "    u_x = jit(grad(nn.forward_squeeze, argnums=2))\n",
    "    squared_residual = jit(lambda params, u, omega, x: (u_x(params, u, x) - omega * jnp.cos(omega * x)) ** 2)\n",
    "    squared_residual_batch = jit(vmap(squared_residual, in_axes=(None, 0, 0, 0))) \n",
    "\n",
    "    # With or without data\n",
    "    # squared_error = jit(lambda params, u: nn.forward(params, u, 0) ** 2)\n",
    "    squared_error = jit(lambda params, u, x, y: (nn.forward(params, u, x) - y) ** 2)\n",
    "    squared_errors_batch = jit(vmap(squared_error, in_axes=(None, 0, 0, 0)))\n",
    "    \n",
    "    # Initialize the Adam optimizer with the learning rate schedule\n",
    "    learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "        init_value=0.01,\n",
    "        boundaries_and_scales={300: 0.2, 2000: 0.5}\n",
    "    )\n",
    "    optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "    opt_state = optimizer.init(nn.params())\n",
    "    losses = []\n",
    "\n",
    "    # Training loop\n",
    "    max_iterations = max_iter\n",
    "\n",
    "    # Training loop with tqdm progress bar\n",
    "    pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "    for epoch in pbar:\n",
    "        don_params, opt_state, current_loss = train_step(nn.params(), opt_state, u, omega, x, y)\n",
    "        nn.branch_net.params = don_params[0] \n",
    "        nn.trunk_net.params = don_params[1]\n",
    "        losses.append(current_loss)\n",
    "        if epoch % 100 == 0:\n",
    "            pbar.set_postfix({'loss': current_loss})\n",
    "        if current_loss < 1.0e-5:\n",
    "            break\n",
    "    \n",
    "    return nn, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = omega_beta\n",
    "\n",
    "# Generate random x values between -1 and 1 with a different random key\n",
    "key_x = random.PRNGKey(4)\n",
    "x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "# Compute y values for the generated omega and x\n",
    "y = jnp.sin(omega * x)\n",
    "\n",
    "n_input_sensor = 2\n",
    "# key_input_sensor = random.PRNGKey(5)\n",
    "# x_input_sensor = random.uniform(key_input_sensor, shape=(n_input_sensor,), minval=-1, maxval=1)\n",
    "\n",
    "# The points should not be symmetrically distributed when having very few sensor points\n",
    "x_sensor_min = -0.7\n",
    "x_sensor_max = 0.9\n",
    "x_input_sensor = np.linspace(x_sensor_min,x_sensor_max,n_input_sensor)\n",
    "\n",
    "u = jnp.cos(omega[:,None] * x_input_sensor) * omega[:,None]\n",
    "\n",
    "max_iterations = 5000\n",
    "\n",
    "nn = initialize_DON(n_input_sensor)\n",
    "\n",
    "nn, losses = train_DON(\n",
    "    nn, u, omega, x, y, max_iterations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_u = 50  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "\n",
    "# key_test = random.PRNGKey(10)\n",
    "# omega_tests = random.uniform(key_test, shape=(n_samples,), minval=omega_min-delta, maxval=omega_max+delta)\n",
    "omega_tests = jnp.linspace(omega_min - delta, omega_max + delta, n_test_u)\n",
    "\n",
    "u_tests = jnp.cos(omega_tests[:,None] * x_input_sensor) * omega_tests[:,None]\n",
    "\n",
    "mse_results = []\n",
    "\n",
    "for i in range(n_test_u):\n",
    "    omega_test_array = jnp.ones_like(x_test) * omega_tests[i]\n",
    "    y_test = jnp.sin(omega_test_array * x_test)\n",
    "    u_test = jnp.tile(u_tests[i], (len(x_test), 1))\n",
    "\n",
    "    mse = np.mean(\n",
    "        (y_test - nn.predict(u_test, x_test).reshape(y_test.shape))\n",
    "        ** 2\n",
    "    )\n",
    "    \n",
    "    mse_results.append(mse)\n",
    "    \n",
    "\n",
    "# Plot MSE vs Omega (optional)\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- First subplot: Omega distributions ---\n",
    "ax1.hist(omega, bins=50, density=True, histtype=\"step\", \n",
    "         linewidth=2, color=colors[0])\n",
    "\n",
    "ax1.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax1.set_ylabel(\"Density\", fontsize=12)\n",
    "ax1.set_title(\"Omega Distribution\", fontsize=14)\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# --- Second subplot: MSE results (log scale) ---\n",
    "ax2.semilogy(omega_tests, mse_results, \"o-\", linewidth=2, \n",
    "             markersize=5, color=colors[0])\n",
    "\n",
    "# Add vertical dotted lines at boundaries\n",
    "ax2.axvline(omega_min, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "ax2.axvline(omega_max, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax2.set_ylabel(\"MSE (log scale)\", fontsize=12)\n",
    "ax2.set_title(\"Prediction Error (Log Scale)\", fontsize=14)\n",
    "ax2.grid(True, linestyle=\"--\", alpha=0.6, which='both')\n",
    "\n",
    "# Add minor grid lines for the log scale\n",
    "ax2.minorticks_on()\n",
    "ax2.grid(which='minor', linestyle=':', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the results for nn\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot the loss over the number of iterations for nn\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].plot(range(len(losses)), losses, label=\"Loss\")\n",
    "axs[0].set_xlabel(\"Iteration\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the final results for nn\n",
    "omega_tests = jnp.linspace(omega_min, omega_max, 5)\n",
    "u_tests = jnp.cos(omega_tests[:,None] * x_input_sensor) * omega_tests[:,None]\n",
    "\n",
    "# omega_tests = omega_tests.at[1].set(0)\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(10, 6))  # Adjust layout if needed\n",
    "\n",
    "for omega_test, color, i in zip(omega_tests, colors, range(5)):\n",
    "    omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "    y_test = jnp.sin(omega_test_array * x_test)\n",
    "    u_test = jnp.tile(u_tests[i], (len(x_test), 1))\n",
    "\n",
    "    # Plot true sine function (dashed line)\n",
    "    axs[1].plot(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Sine function (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    # Plot neural network prediction (solid line)\n",
    "    axs[1].plot(\n",
    "        x_test,\n",
    "        nn.predict(u_test, x_test).reshape(y_test.shape),\n",
    "        color=color,\n",
    "        label=f\"Neural network (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "# Add vertical dotted lines at each sensor point\n",
    "for x_sensor in x_input_sensor:\n",
    "    axs[1].axvline(x=x_sensor, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "# Add legend outside the plot\n",
    "axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Set titles for each subplot\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[1].set_title(\"Solutions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_adjust = 42\n",
    "\n",
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_u = 50  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "omega_tests = jnp.linspace(omega_min - delta, omega_max + delta, n_test_u)\n",
    "\n",
    "# Number of repetitions for training\n",
    "n_input_sensor_list = [1, 2, 4, 8]\n",
    "max_iterations = 4000\n",
    "\n",
    "num_repeats = 5\n",
    "\n",
    "mean_mse_results = []\n",
    "\n",
    "for i in range(len(n_input_sensor_list)): \n",
    "    print(f\"Training repetition {i + 1}/{len(n_input_sensor_list)}\")\n",
    "\n",
    "    # List to store MSE results for each repeat\n",
    "    all_mse_results = []\n",
    "\n",
    "    for j in range(num_repeats):\n",
    "        print(f\"Repetition {j + 1}/{num_repeats}\")\n",
    "\n",
    "        random_key = random.PRNGKey(key_adjust + j)  # Random key for JAX\n",
    "        omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "\n",
    "        omega = omega_beta\n",
    "\n",
    "        # Generate random x values between -1 and 1 with a different random key\n",
    "        key_x = random.PRNGKey(key_adjust + j)\n",
    "        x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "        # Compute y values for the generated omega and x\n",
    "        y = jnp.sin(omega * x)\n",
    "\n",
    "        n_input_sensor = n_input_sensor_list[i]\n",
    "        x_input_sensor = np.linspace(x_sensor_min,x_sensor_max,n_input_sensor)\n",
    "\n",
    "        u = jnp.cos(omega[:, None] * x_input_sensor) * omega[:, None]\n",
    "\n",
    "        u_tests = jnp.cos(omega_tests[:, None] * x_input_sensor) * omega_tests[:, None]\n",
    "\n",
    "        nn = initialize_DON(n_input_sensor)\n",
    "\n",
    "        nn, losses = train_DON(\n",
    "            nn, u, omega, x, y, max_iterations\n",
    "        )\n",
    "\n",
    "        mse_results = []\n",
    "\n",
    "        for j in range(n_test_u):\n",
    "            omega_test_array = jnp.ones_like(x_test) * omega_tests[j]\n",
    "            y_test = jnp.sin(omega_test_array * x_test)\n",
    "            u_test = jnp.tile(u_tests[j], (len(x_test), 1))\n",
    "\n",
    "            mse = np.mean(\n",
    "                (y_test - nn.predict(u_test, x_test).reshape(y_test.shape))\n",
    "                ** 2\n",
    "            )\n",
    "            \n",
    "            mse_results.append(mse)\n",
    "\n",
    "        # Save the MSE results for this repeat\n",
    "        all_mse_results.append(mse_results)\n",
    "\n",
    "    mean_mse_results.append(np.mean(all_mse_results, axis=0))\n",
    "\n",
    "# Plot all MSE results together\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, mean_mse_result in enumerate(mean_mse_results):\n",
    "    plt.semilogy(\n",
    "        omega_tests,\n",
    "        mean_mse_result,\n",
    "        label=f\"Input sensor points={n_input_sensor_list[i]}\",\n",
    "        color=colors[i % len(colors)],\n",
    "        marker=\"o\",\n",
    "        linewidth=2,\n",
    "        markersize=5,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Omega\", fontsize=12)\n",
    "plt.ylabel(\"MSE (log scale)\", fontsize=12)\n",
    "plt.title(\"MSE results for different numbers of sensor points\", fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6, which=\"both\")\n",
    "\n",
    "# Add vertical dotted lines at boundaries\n",
    "plt.axvline(omega_min, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "plt.axvline(omega_max, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch sampling 1D case\n",
    "\n",
    "When training, the loss function is computed form all the data points $(f,x,u(x))_i,i=1,...,N$ at each training step. We now instead sample a random batch of $M<N$ data points uniformly from the training data set without replacement such that\n",
    "\\begin{equation}\n",
    "    \\{(f,x,u(x))_i\\}_{i=1}^N\\mapsto\\{(f,x,u(x))_i\\}_{i=1}^M.\n",
    "\\end{equation}\n",
    "Letting $0<b_r<1$ denote the batch ratio, we have $M=\\lfloor b_r N\\rfloor$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_DON(key=None):\n",
    "    # Use the provided key or default to random.PRNGKey(0)\n",
    "    if key is None:\n",
    "        key = random.PRNGKey(0)\n",
    "\n",
    "    # Create a neural network\n",
    "    branch_layer_sizes = [1, 50, 50, 10]\n",
    "    trunk_layer_sizes = [1, 50, 50, 10]\n",
    "    nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, key)\n",
    "\n",
    "    return nn\n",
    "\n",
    "\n",
    "def train_DON(nn, omega, x, y, max_iter, batch_sampling=False, batch_ratio=0.2):\n",
    "    def loss_fn(params, omega, x, y):\n",
    "        return jnp.mean(squared_residual_batch(params, omega, x)) + jnp.mean(\n",
    "            squared_errors_batch(params, omega, x, y)\n",
    "        )\n",
    "\n",
    "    # Define training step with Adam optimizer\n",
    "    @jit\n",
    "    def train_step(params, opt_state, omega, x, y):\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, omega, x, y)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, opt_state, loss\n",
    "\n",
    "    # Define the loss function\n",
    "    u_x = jit(grad(nn.forward_squeeze, argnums=2))\n",
    "    squared_residual = jit(\n",
    "        lambda params, omega, x: (u_x(params, omega, x) - omega * jnp.cos(omega * x))\n",
    "        ** 2\n",
    "    )\n",
    "    squared_residual_batch = jit(vmap(squared_residual, in_axes=(None, 0, 0)))\n",
    "\n",
    "    # With or without data\n",
    "    # squared_error = jit(lambda params, omega: nn.forward(params, omega, 0) ** 2)\n",
    "    squared_error = jit(\n",
    "        lambda params, omega, x, y: (nn.forward(params, omega, x) - y) ** 2\n",
    "    )\n",
    "    squared_errors_batch = jit(vmap(squared_error, in_axes=(None, 0, 0, 0)))\n",
    "\n",
    "    # Training loop\n",
    "    max_iterations = max_iter\n",
    "\n",
    "    # Training loop with tqdm progress bar with batch sampling\n",
    "    if batch_sampling:\n",
    "        # Initialize the Adam optimizer with the learning rate schedule\n",
    "        learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "            init_value=0.01,\n",
    "            boundaries_and_scales={300: 0.2, 2000: 0.5, 3000: 0.1, 4000: 0.1}\n",
    "            # boundaries_and_scales={300: 0.2, 2000: 0.5},\n",
    "        )\n",
    "        optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "        opt_state = optimizer.init(nn.params())\n",
    "        losses = []\n",
    "\n",
    "        pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "        for i, epoch in enumerate(pbar):\n",
    "            \n",
    "            idx = np.random.choice(len(omega),int(len(omega)*batch_ratio), replace = False)\n",
    "            \n",
    "            don_params, opt_state, current_loss = train_step(\n",
    "                nn.params(), opt_state, omega[idx], x[idx], y[idx]\n",
    "            )\n",
    "\n",
    "            nn.branch_net.params = don_params[0]\n",
    "            nn.trunk_net.params = don_params[1]\n",
    "            losses.append(current_loss)\n",
    "            if epoch % 100 == 0:\n",
    "                pbar.set_postfix({\"loss\": current_loss})\n",
    "            if current_loss < 1.0e-5:\n",
    "                break\n",
    "    else:\n",
    "        # Initialize the Adam optimizer with the learning rate schedule\n",
    "        learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "            init_value=0.01, boundaries_and_scales={300: 0.2, 2000: 0.5}\n",
    "        )\n",
    "        optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "        opt_state = optimizer.init(nn.params())\n",
    "        losses = []\n",
    "\n",
    "        # Training loop without batch sampling\n",
    "        pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "        for i, epoch in enumerate(pbar):\n",
    "            don_params, opt_state, current_loss = train_step(\n",
    "                nn.params(), opt_state, omega, x, y\n",
    "            )\n",
    "            nn.branch_net.params = don_params[0]\n",
    "            nn.trunk_net.params = don_params[1]\n",
    "            losses.append(current_loss)\n",
    "            if epoch % 100 == 0:\n",
    "                pbar.set_postfix({\"loss\": current_loss})\n",
    "            if current_loss < 1.0e-5:\n",
    "                break\n",
    "\n",
    "    return nn, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "omega_min = -1.0\n",
    "omega_max = 3 * np.pi\n",
    "\n",
    "random_key = random.PRNGKey(42)  # Random key for JAX\n",
    "n_samples = 1000\n",
    "omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "\n",
    "omega = omega_beta\n",
    "\n",
    "# Generate random x values between -1 and 1 with a different random key\n",
    "key_x = random.PRNGKey(2)\n",
    "x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "# Compute y values for the generated omega and x\n",
    "y = jnp.sin(omega * x)\n",
    "\n",
    "max_iterations = 5000\n",
    "\n",
    "nn = initialize_DON()\n",
    "\n",
    "nn, losses = train_DON(\n",
    "    nn, omega, x, y, max_iterations, False\n",
    ")\n",
    "\n",
    "nn_batch = initialize_DON()\n",
    "\n",
    "nn_batch, losses_batch = train_DON(\n",
    "    nn_batch, omega, x, y, max_iterations, True, 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_omegas = 50  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "omega_tests = jnp.linspace(omega_min - delta, omega_max + delta, n_test_omegas)\n",
    "\n",
    "mse_results = []\n",
    "mse_batch_results = []\n",
    "\n",
    "for omega_test in omega_tests:\n",
    "    omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "    y_test = jnp.sin(omega_test_array * x_test)\n",
    "\n",
    "    mse = np.mean(\n",
    "        (y_test - nn.predict(omega_test_array, x_test).reshape(y_test.shape)) ** 2\n",
    "    )\n",
    "\n",
    "    mse_batch = np.mean(\n",
    "        (y_test - nn_batch.predict(omega_test_array, x_test).reshape(y_test.shape)) ** 2\n",
    "    )\n",
    "\n",
    "    mse_results.append(mse)\n",
    "    mse_batch_results.append(mse_batch)\n",
    "\n",
    "# Plot MSE vs Omega (optional)\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# --- First subplot: Omega distributions ---\n",
    "ax1.hist(omega, bins=50, density=True, histtype=\"step\", \n",
    "         linewidth=2, label=\"Omega distribution\", color=colors[0])\n",
    "\n",
    "ax1.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax1.set_ylabel(\"Density\", fontsize=12)\n",
    "ax1.set_title(\"Omega Distribution\", fontsize=14)\n",
    "ax1.legend(fontsize=12)\n",
    "ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# --- Second subplot: MSE results (log scale) ---\n",
    "ax2.semilogy(omega_tests, mse_results, \"o-\", linewidth=2, \n",
    "             markersize=5, label=\"Beta\", color=colors[0])\n",
    "ax2.semilogy(omega_tests, mse_batch_results, \"o-\", linewidth=2,\n",
    "             markersize=5, label=\"Batch\", color=colors[1])\n",
    "\n",
    "# Add vertical dotted lines at boundaries\n",
    "ax2.axvline(omega_min, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "ax2.axvline(omega_max, color='k', linestyle=':', alpha=0.7, linewidth=1.5)\n",
    "\n",
    "ax2.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax2.set_ylabel(\"MSE (log scale)\", fontsize=12)\n",
    "ax2.set_title(\"Prediction Error (Log Scale)\", fontsize=14)\n",
    "ax2.legend(fontsize=12)\n",
    "ax2.grid(True, linestyle=\"--\", alpha=0.6, which='both')\n",
    "\n",
    "# Add minor grid lines for the log scale\n",
    "ax2.minorticks_on()\n",
    "ax2.grid(which='minor', linestyle=':', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot the loss over the number of iterations for nn\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(len(losses_batch)), losses_batch, label=\"Loss (batch)\")\n",
    "plt.plot(range(len(losses)), losses, label=\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Loss over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss (log scale)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Plot the results for nn\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(14, 15))\n",
    "\n",
    "# Plot the loss over the number of iterations for nn\n",
    "axs[0].set_yscale(\"log\")\n",
    "axs[0].plot(range(len(losses_batch)), losses_batch, label=\"Loss (batch)\")\n",
    "axs[0].plot(range(len(losses)), losses, label=\"Loss\")\n",
    "axs[0].set_xlabel(\"Iteration\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot the final results for nn\n",
    "omega_tests = jnp.linspace(omega_min, omega_max, 5)\n",
    "# key_uniform = random.PRNGKey(10)\n",
    "# omega_tests = random.uniform(key_uniform, shape=(5,), minval=omega_min, maxval=omega_max)\n",
    "omega_tests = omega_tests.at[1].set(0)\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(10, 6))  # Adjust layout if needed\n",
    "\n",
    "for omega_test, color in zip(omega_tests, colors):\n",
    "    omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "    y_test = jnp.sin(omega_test_array * x_test)\n",
    "    # Plot true sine function (dashed line)\n",
    "    axs[1].plot(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Sine function (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    # Plot neural network prediction (solid line)\n",
    "    axs[1].plot(\n",
    "        x_test,\n",
    "        nn.predict(omega_test_array, x_test).reshape(y_test.shape),\n",
    "        color=color,\n",
    "        label=f\"Neural network (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    axs[2].plot(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        color=color,\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Sine function (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "    # Plot neural network prediction (solid line)\n",
    "    axs[2].plot(\n",
    "        x_test,\n",
    "        nn_batch.predict(omega_test_array, x_test).reshape(y_test.shape),\n",
    "        color=color,\n",
    "        label=f\"Neural network (ω={float(omega_test):.2f})\",  # Round to 2 decimals\n",
    "    )\n",
    "\n",
    "# Add legend outside the plot\n",
    "axs[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "axs[2].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Set titles for each subplot\n",
    "axs[0].set_title(\"Loss\")\n",
    "axs[1].set_title(\"Regular\")\n",
    "axs[2].set_title(\"Batch sampling\") \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_omegas = 50  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "omega_tests = jnp.linspace(omega_min - delta, omega_max + delta, n_test_omegas)\n",
    "\n",
    "# Number of repetitions for training\n",
    "num_repeats = 5\n",
    "\n",
    "all_mse_results = []\n",
    "all_mse_batch_results = []\n",
    "\n",
    "n_samples = 1000\n",
    "max_iterations = 5000\n",
    "max_iterations_batch = 5000\n",
    "\n",
    "batch_ratio = 0.6\n",
    "\n",
    "total_time_nn = 0\n",
    "total_time_nn_batch = 0\n",
    "\n",
    "key_adjust = 42\n",
    "\n",
    "for i in range(num_repeats):\n",
    "    print(f\"Training repetition {i + 1}/{num_repeats}\")\n",
    "\n",
    "    random_key = random.PRNGKey(key_adjust + i)  # Random key for JAX\n",
    "    omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "    omega = omega_beta\n",
    "\n",
    "    # Generate random x values between -1 and 1 with a different random key\n",
    "    key_x = random.PRNGKey(key_adjust + i)\n",
    "    x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "    # Compute y values for the generated omega and x\n",
    "    y = jnp.sin(omega * x)\n",
    "\n",
    "    # Train DON models\n",
    "    random_key = random.PRNGKey(key_adjust + 2 * num_repeats + i)\n",
    "    \n",
    "    # Time training for nn\n",
    "    nn = initialize_DON(random_key)\n",
    "    start_time = time.time()\n",
    "    nn, losses = train_DON(nn, omega, x, y, max_iterations, False)\n",
    "    end_time = time.time()\n",
    "    total_time_nn += (end_time - start_time)\n",
    "\n",
    "    # Time training for nn_batch\n",
    "    nn_batch = initialize_DON(random_key)\n",
    "    start_time = time.time()\n",
    "    nn_batch, losses_batch = train_DON(nn_batch, omega, x, y, max_iterations_batch, True, batch_ratio)\n",
    "    end_time = time.time()\n",
    "    total_time_nn_batch += (end_time - start_time)\n",
    "\n",
    "    # Compute MSE for each omega_test\n",
    "    mse_results = []\n",
    "    mse_batch_results = []\n",
    "\n",
    "    for omega_test in omega_tests:\n",
    "        omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "        y_test = jnp.sin(omega_test_array * x_test)\n",
    "\n",
    "        mse = np.mean(\n",
    "            (y_test - nn.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "            ** 2\n",
    "        )\n",
    "        mse_batch = np.mean(\n",
    "            (y_test - nn_batch.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "            ** 2\n",
    "        )\n",
    "\n",
    "        mse_results.append(mse)\n",
    "        mse_batch_results.append(mse_batch)\n",
    "\n",
    "    # Store results for this repetition\n",
    "    all_mse_results.append(mse_results)\n",
    "    all_mse_batch_results.append(mse_batch_results)\n",
    "\n",
    "# Compute mean MSE across all repetitions for each omega\n",
    "mean_mse = np.mean(all_mse_results, axis=0)\n",
    "mean_mse_batch = np.mean(all_mse_batch_results, axis=0)\n",
    "\n",
    "average_time_nn = total_time_nn / num_repeats\n",
    "average_time_nn_batch = total_time_nn_batch / num_repeats\n",
    "\n",
    "print(f\"Average training time for nn: {average_time_nn:.2f} seconds\")\n",
    "print(f\"Average training time for nn_batch: {average_time_nn_batch:.2f} seconds\")\n",
    "\n",
    "# Plot mean MSE vs Omega\n",
    "colors = [\"b\", \"g\", \"r\"]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot mean MSE results (log scale)\n",
    "ax.semilogy(\n",
    "    omega_tests,\n",
    "    mean_mse,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    "    label=\"Regular\",\n",
    "    color=colors[0],\n",
    ")\n",
    "ax.semilogy(\n",
    "    omega_tests,\n",
    "    mean_mse_batch,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    "    label=\"Batch\",\n",
    "    color=colors[1],\n",
    ")\n",
    "\n",
    "# Add vertical dotted lines at boundaries\n",
    "ax.axvline(omega_min, color=\"k\", linestyle=\":\", alpha=0.7, linewidth=1.5)\n",
    "ax.axvline(omega_max, color=\"k\", linestyle=\":\", alpha=0.7, linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel(\"Omega\", fontsize=12)\n",
    "ax.set_ylabel(\"Mean MSE (log scale)\", fontsize=12)\n",
    "ax.set_title(\"Mean Prediction Error (Log Scale)\", fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6, which=\"both\")\n",
    "\n",
    "# Add minor grid lines for the log scale\n",
    "ax.minorticks_on()\n",
    "ax.grid(which=\"minor\", linestyle=\":\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for x\n",
    "x_test = jnp.linspace(-1.0, 1.0, 1000)\n",
    "\n",
    "# Plot the final results for nn\n",
    "n_test_omegas = 200  # Number of test omega values\n",
    "\n",
    "# Generate equally spaced omega_test values\n",
    "delta = 1\n",
    "omega_trains = jnp.linspace(omega_min, omega_max, n_test_omegas)\n",
    "omega_test_temp1 = jnp.linspace(omega_min - delta, omega_min, int(0.5*n_test_omegas))\n",
    "omega_test_temp2 = jnp.linspace(omega_max, omega_max + delta, int(0.5*n_test_omegas))\n",
    "omega_tests = jnp.concatenate((omega_test_temp1, omega_test_temp2))\n",
    "\n",
    "# Number of repetitions for training\n",
    "batch_ratios = [0.01, 0.1, 0.2, 0.5, 0.8, 0.9, 0.95, 0.99]\n",
    "# batch_ratios = [0.05, 0.1]\n",
    "\n",
    "n_samples = 1000\n",
    "max_iterations = 5000\n",
    "max_iterations_batch = 5000\n",
    "\n",
    "mean_mse_train = []\n",
    "mean_mse_train_batch = []\n",
    "\n",
    "mean_mse_test = []\n",
    "mean_mse_test_batch = []\n",
    "\n",
    "num_repeats = 10\n",
    "\n",
    "key_adjust = 42\n",
    "\n",
    "all_mse_train_results = []\n",
    "all_mse_test_results = []\n",
    "total_time_nn = 0\n",
    "\n",
    "for i in range(num_repeats):\n",
    "    print(f\"Training repetition {i + 1}/{num_repeats}\")\n",
    "\n",
    "    random_key = random.PRNGKey((i+1)*key_adjust + i)  # Random key for JAX\n",
    "    omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "    omega = omega_beta\n",
    "\n",
    "    # Generate random x values between -1 and 1 with a different random key\n",
    "    key_x = random.PRNGKey((i+1)*key_adjust + i)\n",
    "    x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "    # Compute y values for the generated omega and x\n",
    "    y = jnp.sin(omega * x)\n",
    "\n",
    "    # Train DON models\n",
    "    random_key = random.PRNGKey((i+1)*key_adjust + 2 * num_repeats + i)\n",
    "    \n",
    "    # Time training for nn\n",
    "    nn = initialize_DON(random_key)\n",
    "    start_time = time.time()\n",
    "    nn, losses = train_DON(nn, omega, x, y, max_iterations, False)\n",
    "    end_time = time.time()\n",
    "    total_time_nn += (end_time - start_time)\n",
    "\n",
    "    # Compute MSE for each omega_test\n",
    "    mse_test_results = []\n",
    "\n",
    "    mse_train_results = []\n",
    "\n",
    "    for omega_train in omega_trains:\n",
    "\n",
    "        omega_train_array = jnp.ones_like(x_test) * omega_train\n",
    "        y_train = jnp.sin(omega_train_array * x_test)\n",
    "        \n",
    "        mse_train = np.mean(\n",
    "            (y_train - nn.predict(omega_train_array, x_test).reshape(y_train.shape))\n",
    "            ** 2\n",
    "        )\n",
    "\n",
    "        mse_train_results.append(mse_train)\n",
    "\n",
    "    for omega_test in omega_tests:\n",
    "\n",
    "        omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "        y_test = jnp.sin(omega_test_array * x_test)\n",
    "\n",
    "        mse_test = np.mean(\n",
    "            (y_test - nn.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "            ** 2\n",
    "        )\n",
    "        \n",
    "        mse_test_results.append(mse_test)\n",
    "\n",
    "    # Store results for this repetition\n",
    "    all_mse_train_results.append(np.mean(mse_train_results))\n",
    "\n",
    "    all_mse_test_results.append(np.mean(mse_test_results))\n",
    "\n",
    "# Compute mean MSE across all repetitions for each omega\n",
    "mean_mse_train.append(np.mean(all_mse_train_results))\n",
    "\n",
    "mean_mse_test.append(np.mean(all_mse_test_results))\n",
    "\n",
    "average_time_nn = total_time_nn / num_repeats\n",
    "\n",
    "print(f\"Average training time for nn: {average_time_nn:.2f} seconds\")\n",
    "\n",
    "for j in range(len(batch_ratios)):\n",
    "    \n",
    "    batch_ratio = batch_ratios[j]\n",
    "    print(f\"Batch ratio {batch_ratio}\")\n",
    "\n",
    "    all_mse_train_batch_results = []\n",
    "\n",
    "    all_mse_test_batch_results = []\n",
    "\n",
    "    total_time_nn_batch = 0\n",
    "\n",
    "    for i in range(num_repeats):\n",
    "        print(f\"Training repetition {i + 1}/{num_repeats}\")\n",
    "\n",
    "        random_key = random.PRNGKey((j+1)*key_adjust + i)  # Random key for JAX\n",
    "        omega_normal, omega_uniform, omega_beta = generate_omega(omega_min, omega_max, n_samples, random_key)\n",
    "        omega = omega_beta\n",
    "\n",
    "        # Generate random x values between -1 and 1 with a different random key\n",
    "        key_x = random.PRNGKey((j+1)*key_adjust + i)\n",
    "        x = random.uniform(key_x, shape=(n_samples,), minval=-1, maxval=1)\n",
    "\n",
    "        # Compute y values for the generated omega and x\n",
    "        y = jnp.sin(omega * x)\n",
    "\n",
    "        # Train DON models\n",
    "        random_key = random.PRNGKey((j+1)*key_adjust + 2 * num_repeats + i)\n",
    "        \n",
    "        # Time training for nn_batch\n",
    "        nn_batch = initialize_DON(random_key)\n",
    "        start_time = time.time()\n",
    "        nn_batch, losses_batch = train_DON(nn_batch, omega, x, y, max_iterations_batch, True, batch_ratio)\n",
    "        end_time = time.time()\n",
    "        total_time_nn_batch += (end_time - start_time)\n",
    "\n",
    "        # Compute MSE for each omega_test\n",
    "        mse_test_batch_results = []\n",
    "\n",
    "        mse_train_batch_results = []\n",
    "\n",
    "        for omega_train in omega_trains:\n",
    "\n",
    "            omega_train_array = jnp.ones_like(x_test) * omega_train\n",
    "            y_train = jnp.sin(omega_train_array * x_test)\n",
    "            \n",
    "            mse_train_batch = np.mean(\n",
    "                (y_train - nn_batch.predict(omega_train_array, x_test).reshape(y_train.shape))\n",
    "                ** 2\n",
    "            )\n",
    "\n",
    "            mse_train_batch_results.append(mse_train_batch)\n",
    "\n",
    "        for omega_test in omega_tests:\n",
    "\n",
    "            omega_test_array = jnp.ones_like(x_test) * omega_test\n",
    "            y_test = jnp.sin(omega_test_array * x_test)\n",
    "\n",
    "            mse_test_batch = np.mean(\n",
    "                (y_test - nn_batch.predict(omega_test_array, x_test).reshape(y_test.shape))\n",
    "                ** 2\n",
    "            )\n",
    "\n",
    "            mse_test_batch_results.append(mse_test_batch)\n",
    "\n",
    "        # Store results for this repetition\n",
    "        all_mse_train_batch_results.append(np.mean(mse_train_batch_results))\n",
    "\n",
    "        all_mse_test_batch_results.append(np.mean(mse_test_batch_results))\n",
    "\n",
    "    # Compute mean MSE across all repetitions for each omega\n",
    "    mean_mse_train_batch.append(np.mean(all_mse_train_batch_results))\n",
    "\n",
    "    mean_mse_test_batch.append(np.mean(all_mse_test_batch_results))\n",
    "\n",
    "    average_time_nn_batch = total_time_nn_batch / num_repeats\n",
    "\n",
    "    print(f\"Average training time for nn_batch: {average_time_nn_batch:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot mean MSE vs Omega\n",
    "colors = [\"b\", \"g\", \"r\"]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot batch test/train MSE (varying with batch_ratios)\n",
    "ax.semilogy(\n",
    "    batch_ratios,\n",
    "    mean_mse_test_batch,\n",
    "    \"o--\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    "    label=\"Batch $\\Omega_{train}\\setminus \\Omega$\",\n",
    "    color=colors[1],\n",
    ")\n",
    "ax.semilogy(\n",
    "    batch_ratios,\n",
    "    mean_mse_train_batch,\n",
    "    \"o-\",\n",
    "    linewidth=2,\n",
    "    markersize=5,\n",
    "    label=\"Batch $\\omega \\in I_{train}$\",\n",
    "    color=colors[1],\n",
    ")\n",
    "\n",
    "# Plot constant test/train MSE as horizontal lines\n",
    "ax.axhline(\n",
    "    y=mean_mse_test,\n",
    "    color=colors[0],\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=\"Baseline $\\omega \\in I_{test}$\",\n",
    ")\n",
    "ax.axhline(\n",
    "    y=mean_mse_train,\n",
    "    color=colors[0],\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=\"Baseline $\\omega \\in I_{train}$\",\n",
    ")\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_xlabel(\"Batch ratio\", fontsize=12)\n",
    "ax.set_ylabel(\"Mean MSE (log scale)\", fontsize=12)\n",
    "ax.set_title(\"Mean Prediction Error (Log Scale)\", fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6, which=\"both\")\n",
    "\n",
    "# Add minor grid lines for the log scale\n",
    "ax.minorticks_on()\n",
    "ax.grid(which=\"minor\", linestyle=\":\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define weak forms of 1d convection diffusion \n",
    "\n",
    "In this notebook we use finite element as a reference solution\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\phi}{\\partial t} + u \\ \\frac{\\partial \\phi}{\\partial x} - \\epsilon \\frac{\\partial^2 \\phi}{\\partial x^2} & = f(x), \\quad x \\in \\Omega = [0,1], \\quad \\phi(0) = \\phi(1)= 0 \\\\\n",
    "&\\Downarrow \\text{Weak formualtion}\\\\\n",
    "\\int_{\\Omega} ( \\frac{\\partial \\phi}{\\partial t} + u \\ \\frac{\\partial \\phi}{\\partial x} - \\epsilon \\frac{\\partial^2 \\phi}{\\partial x^2}) w dx  & = \\int_{\\Omega} f(x) w dx, \\forall w \\\\ \n",
    "\\Downarrow \\text{Discrete solution: } \\phi_h = \\sum_{j=1}^n \\phi_i N_j \\\\\n",
    "\\int_{\\Omega} ( \\frac{\\partial \\phi_h}{\\partial t} + u \\ \\frac{\\partial \\phi_h}{\\partial x} - \\epsilon \\frac{\\partial^2 \\phi_h}{\\partial x^2}) w_h dx  & = \\int_{\\Omega} f(x) w_h dx, \\forall w_h \\in \\{N_1, N_2, \\dots, N_n\\} \\\\\n",
    "\\Downarrow \\\\\n",
    "\\sum_{j=1}^n \\int_{\\Omega} ( \\frac{\\partial N_j}{\\partial t} N_i  + u \\ \\frac{\\partial N_j}{\\partial x } N_i - \\epsilon \\frac{\\partial^2 N_j}{\\partial x^2} N_i ) dx  & = \\int_{\\Omega} f(x) N_i dx, \\quad  \\forall i = 1, \\dots, n \n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem_M(x, Nj, dNj, Nk, dNk):\n",
    "    return Nj * Nk\n",
    "\n",
    "def problem_B_CD(x, Nj, dNj, Nk, dNk):\n",
    "    return -mu*Nj * dNk - eps*dNj * dNk\n",
    "\n",
    "def problem_L_CD(x, Nj, dNj, f, a_fourier):\n",
    "    return  f(x, a_fourier) * Nj\n",
    "\n",
    "xmin = 0; xmax = 1; N = 15\n",
    "x = np.linspace(xmin,xmax, N)\n",
    "mu =  1\n",
    "eps = 0.1\n",
    "p = 2\n",
    "k = 1\n",
    "neval = 4\n",
    "\n",
    "def u_0(x):\n",
    "    return jnp.sin(np.pi * x)\n",
    "\n",
    "a_fourier = np.random.choice(np.linspace(-1,1, 100), 4)\n",
    "\n",
    "def f(x, a_fourier):\n",
    "    basis_fourier = jnp.array([jnp.sin(l * jnp.pi * x) for l in range(1,5)])\n",
    "    return jnp.dot(a_fourier, basis_fourier)\n",
    "    \n",
    "def solve_conv_diff(p,k, x, f, a_fourier, neval = 10):\n",
    "    mesh = create_mesh(x)\n",
    "    space = create_fe_space(deg = p,reg = k, mesh = mesh)\n",
    "    param_map = create_param_map(mesh)\n",
    "    ref_data = create_ref_data(neval = neval, deg = p, integrate = True)\n",
    "\n",
    "    N_time = mesh['m']*neval\n",
    "    dt = 1/N_time\n",
    "\n",
    "    bc = [0,0]\n",
    "    \n",
    "    u_init = projection(mesh, space, ref_data, param_map, u_0)\n",
    "\n",
    "    M,_ = assemble_fe_problem(mesh, space, ref_data, param_map, problem_M, problem_L_CD, f,a_fourier, bc)\n",
    "    S, F = assemble_fe_problem(mesh,space, ref_data, param_map, problem_B_CD, problem_L_CD, f, a_fourier,bc)\n",
    "    \n",
    "    M = csc_matrix(1/dt * M)\n",
    "    S = csc_matrix(S)\n",
    "    U = np.zeros((N_time, space['n']))\n",
    "    \n",
    "    U[0] = u_init\n",
    "    U[:,0] = 0\n",
    "    U[:,-1] = 0\n",
    "    t = np.zeros(N_time)\n",
    "    U_eval = []\n",
    "    dU_dx_eval = []\n",
    "    \n",
    "    x_eval, u_eval, du_dx_eval = evaluate_solution(p, k, neval, u_init, x)\n",
    "    U_eval.append(u_eval)\n",
    "    dU_dx_eval.append(du_dx_eval) \n",
    "    A = M - S\n",
    "    for i in range(1, N_time):\n",
    "        t[i] = t[i-1] + dt \n",
    "        rhs =  M.dot(U[i-1][1:-1]) + F \n",
    "        u_inner = spsolve(A, rhs)\n",
    "        U[i, 1:-1] = u_inner\n",
    "        x_eval, u_eval, du_dx_eval = evaluate_solution(p, k, neval, U[i], x)\n",
    "        U_eval.append(u_eval) \n",
    "        dU_dx_eval.append(du_dx_eval) \n",
    "    return U_eval, dU_dx_eval, x_eval, t \n",
    "\n",
    "U_eval, dU_dx, x_eval, t_eval  = solve_conv_diff(p,k,x,f, a_fourier)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.array(U_eval[:]).T, aspect='auto', origin='lower', cmap='hot')\n",
    "plt.colorbar()\n",
    "\n",
    "print(np.shape(U_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline FeedForward Neural Network \n",
    "\n",
    "\n",
    "Initialize baseline FeedForward neural netowork for mimicking the finite element solver. For the implementation of PINN's and neural operatos we will build upon the baseline nn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, layer_sizes, key, activation_fn = jax.nn.relu):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.key = key\n",
    "        self.activation_fn = activation_fn \n",
    "        self.params = self.initialize_params(layer_sizes, key)\n",
    "    def initialize_params(self, layer_sizes, key):\n",
    "        params = []\n",
    "        keys = random.split(key, len(layer_sizes) -1)\n",
    "        for i in range(len(layer_sizes) -1):\n",
    "            W_key, b_key = random.split(keys[i])\n",
    "            W = random.normal(W_key, (layer_sizes[i], layer_sizes[i+1])) * jnp.sqrt(2.0/layer_sizes[i]) #weights\n",
    "            b = jnp.zeros(layer_sizes[i+1]) #biases\n",
    "            params.append((W,b))\n",
    "        return params\n",
    "    \n",
    "    def forward(self, params, x):\n",
    "        for W,b in params[:-1]:\n",
    "            x = jnp.dot(x,W) + b \n",
    "            x = self.activation_fn(x)\n",
    "        \n",
    "        W,b = params[-1]\n",
    "        x = jnp.dot(x,W) + b\n",
    "        return x \n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(self.params, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax \n",
    "from tqdm import trange, tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit,vmap  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepONet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepONet:\n",
    "    def __init__(self, branch_layer_sizes, trunk_layer_sizes, key, activation_fn=jax.nn.tanh):\n",
    "        # Initialize branch and trunk networks\n",
    "        branch_key, trunk_key = random.split(key)\n",
    "        self.branch_net = FeedForwardNN(branch_layer_sizes, branch_key, activation_fn)\n",
    "        self.trunk_net = FeedForwardNN(trunk_layer_sizes, trunk_key, activation_fn)\n",
    "\n",
    "    def params(self):\n",
    "        return [self.branch_net.params, self.trunk_net.params]\n",
    "    def set_params(self, new_params):\n",
    "        self.branch_net.params = new_params[0]\n",
    "        self.trunk_net.params = new_params[1]\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward(self, params, branch_input, trunk_input):\n",
    "        # Forward pass through branch and trunk networks\n",
    "        branch_output = self.branch_net.forward(params[0], branch_input)\n",
    "        trunk_output = self.trunk_net.forward(params[1], trunk_input)\n",
    "        # Combine outputs using inner product\n",
    "        return jnp.dot(branch_output, trunk_output.T)\n",
    "    \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def forward_squeeze(self, params, branch_input, trunk_input):\n",
    "        return self.forward(params, branch_input, trunk_input).squeeze()\n",
    "\n",
    "    def predict(self, branch_input, trunk_input):\n",
    "        # Predict output for given inputs\n",
    "        return vmap(self.forward, in_axes=(None, 0, 0))(self.params(), branch_input, trunk_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate train and validation dataset \n",
    "\n",
    "Each row in the datasets contains the right hand sise at some input sensors, some randomly chosen variables $x_i, t_i$ where we want to evaluate the solution, and the ground truth $y_i$ which is the finite element solution at point $x_i, t_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_data(p = 2, k = 1, n_input_sensors = 32, N_elements = 10, neval = 10, N_samples = 100, P = 100, M = 1, seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    x_sensors = np.linspace(0,1, n_input_sensors)\n",
    "    x = np.linspace(0,1, N_elements)\n",
    "\n",
    "    forcing_terms = []\n",
    "    solutions = []\n",
    "    X_evaluations = []\n",
    "    T_evaluations = []\n",
    "    a_coef = []\n",
    "    jit\n",
    "    def f(x, a_fourier):\n",
    "        basis_fourier = jnp.array([jnp.sin(ell * jnp.pi * x) for ell in range(1,5)])\n",
    "        return jnp.dot(a_fourier, basis_fourier)\n",
    "\n",
    "    a_coefs = np.random.choice(np.linspace(-M,M, 100), (N_samples, 4))\n",
    "    for iter in range(N_samples):\n",
    "        print(f\"\\r{iter}\", end='', flush=True)\n",
    "        a_fourier = a_coefs[iter]\n",
    "        U_eval, dU_dx_eval, X_eval, t = solve_conv_diff(p, k, x, f, a_fourier, neval)\n",
    "        forcing_terms.append(f(x_sensors, a_fourier))\n",
    "        solutions.append(U_eval)\n",
    "        X_evaluations.append(X_eval)\n",
    "        T_evaluations.append(t)\n",
    "\n",
    "\n",
    "    data = {'f':[],'x':[],'t':[],'y':[],'a_coef':[]}\n",
    "    for k in range(N_samples):\n",
    "        print(f\"\\r{k}\", end='', flush=True)\n",
    "        for j in range(P):\n",
    "            data['f'].append(forcing_terms[k])\n",
    "            idx_t = np.random.randint(0, len(T_evaluations[k]))\n",
    "            idx_x = np.random.randint(0, len(X_evaluations[k]))\n",
    "            data['y'].append(solutions[k][idx_t][idx_x])\n",
    "            data['t'].append(T_evaluations[k][idx_t])\n",
    "            data['x'].append(X_evaluations[k][idx_x])    \n",
    "            data['a_coef'].append(a_coefs[k])\n",
    "    return data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = generate_data(n_input_sensors=32, seed = 42)\n",
    "\n",
    "validation_data = generate_data(n_input_sensors=32, seed = 43)\n",
    "\n",
    "test_data = generate_data(n_input_sensors=32, seed = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def get_data_dir():\n",
    "    \"\"\"Returns the correct data directory path whether in script or notebook\"\"\"\n",
    "    try:\n",
    "        # Works in regular Python files\n",
    "        return Path(__file__).parent.parent / \"data\"\n",
    "    except NameError:\n",
    "        # Fallback for notebooks - assumes you're running from code/ directory\n",
    "        return Path.cwd().parent / \"data\"\n",
    "\n",
    "DATA_DIR = get_data_dir()\n",
    "\n",
    "def save_to_data(data, filename=\"train_data.pkl\"):\n",
    "    \"\"\"Save data to the root-level data directory\"\"\"\n",
    "    DATA_DIR.mkdir(exist_ok=True)  # Creates if doesn't exist\n",
    "    filepath = DATA_DIR / filename\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Data saved to: {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def load_from_data(filename=\"train_data.pkl\"):\n",
    "    \"\"\"Load data from the root-level data directory\"\"\"\n",
    "    filepath = DATA_DIR / filename\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_data(train_data, \"train_data_batch_size.pkl\")\n",
    "save_to_data(validation_data, \"validation_data_batch_size.pkl\")\n",
    "save_to_data(test_data, \"test_data_batch_size.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import optax \n",
    "from tqdm import trange, tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DON(nn, train_data, validation_data, mse = True,  batch_size = 1, max_iterations = 10000):\n",
    "    f_train = jnp.array(train_data['f'])\n",
    "    t_train = jnp.array(train_data['t'])\n",
    "    x_train = jnp.array(train_data['x'])\n",
    "    y_train = jnp.array(train_data['y'])\n",
    "    a_train = jnp.array(train_data['a_coef'])\n",
    "    train_losses = []\n",
    "\n",
    "    validation_losses = []\n",
    "    f_validation = jnp.array(validation_data['f'])\n",
    "    t_validation = jnp.array(validation_data['t'])\n",
    "    x_validation = jnp.array(validation_data['x'])\n",
    "    y_validation = jnp.array(validation_data['y'])\n",
    "    a_validation = jnp.array(validation_data['a_coef'])\n",
    "    \n",
    "    learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "        init_value=0.01,\n",
    "        boundaries_and_scales={2000: 0.5, 4000: 0.5, 6000 :0.1, 8000:0.1}\n",
    "    )\n",
    "\n",
    "    optimizer = optax.adam(learning_rate=learning_rate_schedule)\n",
    "    opt_state = optimizer.init(nn.params())\n",
    "    \n",
    "    pbar = trange(max_iterations, desc=\"Training\", leave=True)\n",
    "\n",
    "    jit\n",
    "    def u_nn(params, f_sample, x, t): \n",
    "        return nn.forward(params, f_sample, jnp.stack([x, t], axis=-1))\n",
    "\n",
    "    u_nn_t = jax.grad(u_nn, argnums= 3)\n",
    "    u_nn_x = jax.grad(u_nn, argnums= 2)\n",
    "    u_nn_xx = jax.grad(u_nn_x, argnums= 2)\n",
    "\n",
    "    jit \n",
    "    def residual(params, f_sample, x, t, a):\n",
    "        basis_fourier = jnp.array([jnp.sin(ell * jnp.pi * x) for ell in range(1,5)])\n",
    "        return u_nn_t(params, f_sample, x, t) + mu * u_nn_x(params, f_sample, x, t) - eps * u_nn_xx(params, f_sample, x, t) -  jnp.dot(a, basis_fourier)\n",
    "\n",
    "    residual_batch = jit(vmap(residual, in_axes=(None, 0, 0, 0, 0)))\n",
    "\n",
    "    jit\n",
    "    def initial_loss(params, f, x):\n",
    "        return u_nn(params, f, x,0) - u_0(x)\n",
    "\n",
    "    jit\n",
    "    def boundary0_loss(params, f, t):\n",
    "        return u_nn(params, f, 0,t) - 0 \n",
    "\n",
    "    jit\n",
    "    def boundary1_loss(params, f, t):\n",
    "        return u_nn(params, f, 1,t) - 0 \n",
    "\n",
    "    initial_loss_batch = jit(vmap(initial_loss, in_axes=(None, 0, 0)))\n",
    "\n",
    "    boundary0_loss_batch = jit(vmap(boundary0_loss, in_axes=(None, 0, 0)))\n",
    "    boundary1_loss_batch = jit(vmap(boundary1_loss, in_axes=(None, 0, 0)))\n",
    "\n",
    "    jit\n",
    "    def squared_error(params, f_sample, x, t, y, a):\n",
    "        return (u_nn(params, f_sample, x, t) - y)**2\n",
    "\n",
    "    squared_error_batch = jit(vmap(squared_error, in_axes =(None, 0, 0, 0, 0, 0))) \n",
    "\n",
    "    jit \n",
    "    def physics_loss(params, f_sample, x, t, y, a):\n",
    "        residual = jnp.mean(residual_batch(params, f_sample, x, t, a)**2)\n",
    "        initial = jnp.mean(initial_loss_batch(params, f_sample, x)**2)\n",
    "        boundary_0 = jnp.mean(boundary0_loss_batch(params, f_sample, t)**2)\n",
    "        boundary_1 = jnp.mean(boundary1_loss_batch(params, f_sample, t)**2)\n",
    "        return residual + 10*(initial + boundary_0 + boundary_1)\n",
    "\n",
    "    jit \n",
    "    def data_loss(params, f_sample, x, t, y, a):\n",
    "        return jnp.mean(squared_error_batch(params, f_sample, x, t, y,a))\n",
    "\n",
    "    jit\n",
    "    def combined_loss(params, f_sample, x, t, y, a):\n",
    "        return data_loss(params, f_sample, x, t, y, a) + physics_loss(params, f_sample, x, t, y, a) \n",
    "    \n",
    "    jit \n",
    "    def train_step(params, opt_state, f_sample, x, t, y, a, loss_function, optimizer):\n",
    "        loss, grads = jax.value_and_grad(loss_function)(params, f_sample, x, t, y, a)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        new_params = optax.apply_updates(params, updates)\n",
    "        return new_params, opt_state, loss\n",
    "    if (mse):\n",
    "        loss_function = data_loss\n",
    "    else:\n",
    "        loss_function = combined_loss \n",
    "    for epoch in pbar:\n",
    "        idicies = np.random.choice(len(f_train),int(len(f_train)*batch_size), replace = False) #extract random batches from train data \n",
    "        don_params, opt_state, current_train_loss = train_step(nn.params(), opt_state, f_train[idicies], x_train[idicies], t_train[idicies], y_train[idicies], a_train[idicies], loss_function, optimizer)\n",
    "        current_val_loss = loss_function(nn.params(), f_validation, x_validation, t_validation, y_validation, a_validation)\n",
    "        nn.branch_net.params = don_params[0] \n",
    "        nn.trunk_net.params = don_params[1]\n",
    "        train_losses.append(current_train_loss)\n",
    "        validation_losses.append(current_val_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            pbar.set_postfix({'train_loss': current_train_loss, 'validation_loss': current_val_loss})\n",
    "\n",
    "    return nn.params(), train_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_batch_size = {}\n",
    "train_loss_batch_size = {}\n",
    "validation_loss_batch_size = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "n_input_sensors = 32\n",
    "branch_layer_sizes = [n_input_sensors,64, 64, 10]\n",
    "trunk_layer_sizes = [2, 64, 64, 10]\n",
    "nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, key)\n",
    "max_iter = 10000\n",
    "params_batch_size_02, train_losses_batch_size_02, validation_losses_batch_size_02 = train_DON(nn, train_data, validation_data, mse = False, batch_size = 0.2, max_iterations= max_iter)\n",
    "params_batch_size['0.2'] = params_batch_size_02\n",
    "train_loss_batch_size['0.2'] = train_losses_batch_size_02\n",
    "validation_loss_batch_size['0.2'] = validation_losses_batch_size_02\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "branch_layer_sizes = [n_input_sensors,64, 64, 10]\n",
    "trunk_layer_sizes = [2, 64, 64, 10]\n",
    "nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, key)\n",
    "\n",
    "params_batch_size_05, train_losses_batch_size_05, validation_losses_batch_size_05 = train_DON(nn, train_data, validation_data, mse=False, batch_size = 0.5, max_iterations= max_iter)\n",
    "params_batch_size['0.5'] = params_batch_size_05\n",
    "train_loss_batch_size['0.5'] = train_losses_batch_size_05\n",
    "validation_loss_batch_size['0.5'] = validation_losses_batch_size_05\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "branch_layer_sizes = [n_input_sensors,64, 64, 10]\n",
    "trunk_layer_sizes = [2, 64, 64, 10]\n",
    "nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, key)\n",
    "\n",
    "\n",
    "params_batch_size_1, train_losses_batch_size_1, validation_losses_batch_size_1 = train_DON(nn, train_data, validation_data, mse=False, batch_size = 1.0, max_iterations= max_iter)\n",
    "params_batch_size['1'] = params_batch_size_1\n",
    "train_loss_batch_size['1'] = train_losses_batch_size_1\n",
    "validation_loss_batch_size['1'] = validation_losses_batch_size_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "save_to_data(params_batch_size, \"params_batch_size.pkl\")\n",
    "save_to_data(train_loss_batch_size, \"train_loss_batch_size.pkl\")\n",
    "save_to_data(validation_loss_batch_size, \"validation_loss_batch_size.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_extracted = load_from_data(\"params_batch_size.pkl\")\n",
    "\n",
    "train_loss_extracted = load_from_data(\"train_loss_batch_size.pkl\")\n",
    "\n",
    "validation_loss_extracted = load_from_data(\"validation_loss_batch_size.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize = (16,5))\n",
    "print(train_loss_extracted.keys())\n",
    "max_ylim = 0\n",
    "min_ylim = 10000\n",
    "for key in train_loss_batch_size.keys():\n",
    "    axes[0].plot(train_loss_extracted[key], label = 'Batch size ' +  key)\n",
    "    axes[1].plot(validation_loss_extracted[key], label = 'Batch size ' + key)\n",
    "    max_ylim = np.max([max_ylim, np.max(train_loss_extracted[key]), np.max(validation_loss_extracted[key])])\n",
    "    min_ylim = np.min([min_ylim, np.min(train_loss_extracted[key]), np.min(validation_loss_extracted[key])])\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[0].set_yscale('log')\n",
    "axes[1].set_yscale('log')\n",
    "axes[0].grid(True)\n",
    "axes[1].grid(True)\n",
    "axes[0].set_ylim([min_ylim,max_ylim])\n",
    "axes[1].set_ylim([min_ylim,max_ylim])\n",
    "axes[0].set_title(f'Train loss')\n",
    "axes[1].set_title(f'Validation loss')\n",
    "fig.suptitle(f'Log convergence plot for batch size')\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'log_convergence_batch_size.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_batch_size = load_from_data(\"test_data_batch_size.pkl\")\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "branch_layer_sizes = [n_input_sensors,64, 64, 10]\n",
    "trunk_layer_sizes = [2, 64, 64, 10]\n",
    "\n",
    "nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, key)\n",
    "\n",
    "u_nn = jax.jit(lambda params, f, x, t : nn.forward(params, f, jnp.stack([x, t], axis=-1)))\n",
    "\n",
    "for key in params_batch_size.keys():\n",
    "    jit\n",
    "    def squared_error(params, f_sample, x, t, y, a):\n",
    "        return (u_nn(params, f_sample, x, t) - y)**2\n",
    "\n",
    "    squared_error_batch = jit(vmap(squared_error, in_axes =(None, 0, 0, 0, 0, 0))) \n",
    "    jit \n",
    "    def data_loss(params, f_sample, x, t, y, a):\n",
    "        return jnp.mean(squared_error_batch(params, f_sample, x, t, y,a))\n",
    "\n",
    "    print(key, f'{data_loss(params_batch_size[key], jnp.array(test_data_batch_size['f']), jnp.array(test_data_batch_size['x']), jnp.array(test_data_batch_size['t']),jnp.array(test_data_batch_size['y']), jnp.array(test_data_batch_size['a_coef'])):.2e}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_15 = generate_data(M = 1.5)\n",
    "save_to_data(test_data_15, \"test_data_15_batch_size.pkl\")\n",
    "key = random.PRNGKey(0)\n",
    "branch_layer_sizes = [n_input_sensors,64, 64, 10]\n",
    "trunk_layer_sizes = [2, 64, 64, 10]\n",
    "\n",
    "nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, key)\n",
    "\n",
    "u_nn = jax.jit(lambda params, f, x, t : nn.forward(params, f, jnp.stack([x, t], axis=-1)))\n",
    "\n",
    "for key in params_batch_size.keys():\n",
    "    jit\n",
    "    def squared_error(params, f_sample, x, t, y, a):\n",
    "        return (u_nn(params, f_sample, x, t) - y)**2\n",
    "\n",
    "    squared_error_batch = jit(vmap(squared_error, in_axes =(None, 0, 0, 0, 0, 0))) \n",
    "    jit \n",
    "    def data_loss(params, f_sample, x, t, y, a):\n",
    "        return jnp.mean(squared_error_batch(params, f_sample, x, t, y,a))\n",
    "\n",
    "    print(key, f'{data_loss(params_batch_size[key], jnp.array(test_data_15['f']), jnp.array(test_data_15['x']), jnp.array(test_data_15['t']),jnp.array(test_data_15['y']), jnp.array(test_data_15['a_coef'])):.2e}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investiagtion of number of input sensors for DeepONet for 1D time dependent convection diffusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = generate_data(n_input_sensors=32, seed = 42)\n",
    "print(f'----')\n",
    "validation_data = generate_data(n_input_sensors=32, seed = 43)\n",
    "print(f'----')\n",
    "test_data = generate_data(n_input_sensors=32, seed = 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_data(train_data, \"train_data_input_sensors.pkl\")\n",
    "save_to_data(validation_data, \"validation_data_input_sensors.pkl\")\n",
    "save_to_data(test_data, \"test_data_input_sensors.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Losses_Data = {}\n",
    "Validation_Losses_Data = {}\n",
    "\n",
    "Train_Losses_Combined = {}\n",
    "Validation_Losses_Combined = {}\n",
    "\n",
    "params_from_data_loss = {}\n",
    "params_from_combined_loss = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a_fourier):\n",
    "    basis_fourier = jnp.array([jnp.sin(l * jnp.pi * x) for l in range(1,5)])\n",
    "    return jnp.dot(a_fourier, basis_fourier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_iterations = 10000\n",
    "\n",
    "for n_input_sensors in [2, 4, 5, 6, 8, 16, 32]:\n",
    "    branch_layer_sizes = [n_input_sensors, 64, 64, 10]\n",
    "    trunk_layer_sizes = [2, 64, 64, 10]\n",
    "    x_sensors = np.linspace(0,1, n_input_sensors)\n",
    "    forcing_terms_train = []\n",
    "    for a in train_data['a_coef']:\n",
    "        a_fourier = np.copy(a)\n",
    "        forcing_terms_train.append(f(x_sensors, a_fourier))\n",
    "\n",
    "    train_data['f'] = forcing_terms_train\n",
    "\n",
    "    forcing_terms_validation = []\n",
    "    for a in validation_data['a_coef']:\n",
    "        a_fourier = np.copy(a)\n",
    "        forcing_terms_validation.append(f(x_sensors, a_fourier))\n",
    "\n",
    "    validation_data['f'] = forcing_terms_validation\n",
    "    \n",
    "\n",
    "    learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "        init_value=0.01,\n",
    "        boundaries_and_scales={1000: 0.5, 3000: 0.5, 4000 :0.5, 5000:0.5}\n",
    "    )\n",
    "\n",
    "    # learning_rate_schedule = optax.piecewise_constant_schedule(\n",
    "    #     init_value=0.001,\n",
    "    #     boundaries_and_scales={1000: 1}\n",
    "    # )\n",
    "\n",
    "    print(f'----- Number of input sensors: {n_input_sensors} -----')\n",
    "    nn_combined = DeepONet(branch_layer_sizes, trunk_layer_sizes, random.PRNGKey(n_input_sensors))\n",
    "    #nn_physics = DeepONet(branch_layer_sizes, trunk_layer_sizes, random.PRNGKey(1))\n",
    "    \n",
    "    \n",
    "    params_combined, train_losses_combined, validation_losses_combined = train_DON(nn_combined, train_data, validation_data, mse = False, batch_size=0.2, max_iterations=max_iterations)\n",
    "    params_from_combined_loss[n_input_sensors] = params_combined\n",
    "    Train_Losses_Combined[n_input_sensors] = train_losses_combined \n",
    "    Validation_Losses_Combined[n_input_sensors] = validation_losses_combined \n",
    "    \n",
    "    # nn_data = DeepONet(branch_layer_sizes, trunk_layer_sizes, random.PRNGKey(n_input_sensors +1))\n",
    "    # params_data, train_losses_data, validation_losses_data = train_DON(nn_data, train_data, validation_data, data_loss, batch_size=0.2, max_iterations=max_iterations) \n",
    "    # params_from_data_loss[n_input_sensors] = params_data\n",
    "    # Train_Losses_Data[n_input_sensors] = train_losses_data \n",
    "    # Validation_Losses_Data[n_input_sensors] = validation_losses_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open(\"params_input_sensors.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(params_from_combined_loss, file)\n",
    "\n",
    "save_to_data(params_from_combined_loss, \"params_input_sensors.pkl\")\n",
    "\n",
    "# with open(\"train_losses_combined_input_sensors.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(Train_Losses_Combined, file)\n",
    "\n",
    "save_to_data(Train_Losses_Combined, \"train_losses_combined_input_sensors.pkl\")\n",
    "\n",
    "# with open(\"validation_losses_combined_input_sensors.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(Validation_Losses_Combined, file)\n",
    "\n",
    "save_to_data(Validation_Losses_Combined, \"validation_losses_combined_input_sensors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_extracted = load_from_data(\"params_input_sensors.pkl\")\n",
    "\n",
    "train_losses_extracted = load_from_data(\"train_losses_combined_input_sensors.pkl\")\n",
    "\n",
    "validation_losses_extracted = load_from_data(\"validation_losses_combined_input_sensors.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize = (16,5))\n",
    "\n",
    "ymin = 100\n",
    "ymax = 0 \n",
    "for n_input_sensors in params_from_combined_loss.keys():\n",
    "    # print(n_input_sensors)\n",
    "    # print([np.min( Validation_Losses_Combined[n_input_sensors]), np.min(Train_Losses_Combined[n_input_sensors]), np.min(Validation_Losses_Data[n_input_sensors]), np.min(Train_Losses_Data[n_input_sensors])])\n",
    "    # print([np.max( Validation_Losses_Combined[n_input_sensors]), np.max(Train_Losses_Combined[n_input_sensors]), np.max(Validation_Losses_Data[n_input_sensors]), np.max(Train_Losses_Data[n_input_sensors])])\n",
    "    ymin = np.min([ymin, np.min(train_losses_extracted[n_input_sensors]), np.min(validation_losses_extracted[n_input_sensors])])\n",
    "    ymax = np.max([ymax, np.max( validation_losses_extracted[n_input_sensors]), np.max(train_losses_extracted[n_input_sensors])])\n",
    "\n",
    "    axes[0].plot(train_losses_extracted[n_input_sensors], label = r'$n_s = $' + f'{n_input_sensors}')\n",
    "    # axes[0].plot(Train_Losses_Physics[n_input_sensors],label = f'physics loss')\n",
    "\n",
    "    axes[1].plot(validation_losses_extracted[n_input_sensors], label = r'$n_s = $' + f'{n_input_sensors}')\n",
    "    # axes[1].plot(Validation_Losses_Physics[n_input_sensors],label = f'physics loss')\n",
    "\n",
    "\n",
    "axes[0].set_ylim(ymin, ymax)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_title('Train loss')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].set_ylim(ymin, ymax)\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_title('Validation loss')\n",
    "\n",
    "axes[1].legend()\n",
    "axes[0].grid(True)\n",
    "axes[1].grid(True)\n",
    "fig.suptitle(f'Log convergence plot for number of input sensors')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(f'log_convergence_input_sensors.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a_fourier):\n",
    "    basis_fourier = jnp.array([jnp.sin(l * jnp.pi * x) for l in range(1,5)])\n",
    "    return jnp.dot(a_fourier, basis_fourier)\n",
    "\n",
    "\n",
    "MSE_test_1 = []\n",
    "for n_input_sensors in params_from_combined_loss.keys():\n",
    "    branch_layer_sizes = [n_input_sensors, 64, 64, 10]\n",
    "    trunk_layer_sizes = [2, 64, 64, 10]\n",
    "    x_sensors = np.linspace(0,1, n_input_sensors)\n",
    "    forcing_terms_test = []\n",
    "    nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, random.PRNGKey(n_input_sensors))\n",
    "    nn.set_params(params_extracted[n_input_sensors])\n",
    "    for a in test_data['a_coef']:\n",
    "        a_fourier = np.copy(a)\n",
    "        forcing_terms_test.append(f(x_sensors, a_fourier))\n",
    "\n",
    "    test_data['f'] = forcing_terms_test\n",
    "    u_nn = jax.jit(lambda params, f, x, t : nn.forward(params, f, jnp.stack([x, t], axis=-1)))\n",
    "\n",
    "    u_nn_t = jax.grad(u_nn, argnums= 3)\n",
    "    u_nn_x = jax.grad(u_nn, argnums= 2)\n",
    "    u_nn_xx = jax.grad(u_nn_x, argnums= 2)\n",
    "    jit\n",
    "    def squared_error(params, f_sample, x, t, y, a):\n",
    "        return (u_nn(params, f_sample, x, t) - y)**2\n",
    "\n",
    "    squared_error_batch = jit(vmap(squared_error, in_axes =(None, 0, 0, 0, 0, 0))) \n",
    "    jit \n",
    "    def data_loss(params, f_sample, x, t, y, a):\n",
    "        return jnp.mean(squared_error_batch(params, f_sample, x, t, y,a))\n",
    "    print(n_input_sensors, f'{data_loss(nn.params(), jnp.array(test_data['f']), jnp.array(test_data['x']), jnp.array(test_data['t']),jnp.array(test_data['y']), jnp.array(test_data['a_coef'])):.4f}')\n",
    "    MSE_test_1.append(data_loss(nn.params(), jnp.array(test_data['f']), jnp.array(test_data['x']), jnp.array(test_data['t']),jnp.array(test_data['y']), jnp.array(test_data['a_coef'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test for fourier coeffcients $|a_n| \\leq 1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_15 = generate_data(M = 1.5, seed = 45)\n",
    "save_to_data(test_data_15, \"test_data_15_input_sensors.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_15_extracted = load_from_data(\"test_data_15_input_sensors.pkl\")\n",
    "def f(x, a_fourier):\n",
    "    basis_fourier = jnp.array([jnp.sin(l * jnp.pi * x) for l in range(1,5)])\n",
    "    return jnp.dot(a_fourier, basis_fourier)\n",
    "\n",
    "MSE_test_2 = []\n",
    "for n_input_sensors in params_from_combined_loss.keys():\n",
    "    branch_layer_sizes = [n_input_sensors, 64, 64, 10]\n",
    "    trunk_layer_sizes = [2, 64, 64, 10]\n",
    "    x_sensors = np.linspace(0,1, n_input_sensors)\n",
    "    forcing_terms_test = []\n",
    "    nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, random.PRNGKey(n_input_sensors))\n",
    "    nn.set_params(params_extracted[n_input_sensors])\n",
    "    for a in test_data_15_extracted['a_coef']:\n",
    "        a_fourier = np.copy(a)\n",
    "        forcing_terms_test.append(f(x_sensors, a_fourier))\n",
    "\n",
    "    test_data_15_extracted['f'] = forcing_terms_test\n",
    "    u_nn = jax.jit(lambda params, f, x, t : nn.forward(params, f, jnp.stack([x, t], axis=-1)))\n",
    "\n",
    "    u_nn_t = jax.grad(u_nn, argnums= 3)\n",
    "    u_nn_x = jax.grad(u_nn, argnums= 2)\n",
    "    u_nn_xx = jax.grad(u_nn_x, argnums= 2)\n",
    "    jit\n",
    "    def squared_error(params, f_sample, x, t, y, a):\n",
    "        return (u_nn(params, f_sample, x, t) - y)**2\n",
    "\n",
    "    squared_error_batch = jit(vmap(squared_error, in_axes =(None, 0, 0, 0, 0, 0))) \n",
    "    jit \n",
    "    def data_loss(params, f_sample, x, t, y, a):\n",
    "        return jnp.mean(squared_error_batch(params, f_sample, x, t, y,a))\n",
    "    print(n_input_sensors, data_loss(nn.params(), jnp.array(test_data_15_extracted['f']), jnp.array(test_data_15_extracted['x']), jnp.array(test_data_15_extracted['t']),jnp.array(test_data_15_extracted['y']), jnp.array(test_data_15_extracted['a_coef'])))\n",
    "    MSE_test_2.append(data_loss(nn.params(), jnp.array(test_data_15_extracted['f']), jnp.array(test_data_15_extracted['x']), jnp.array(test_data_15_extracted['t']),jnp.array(test_data_15_extracted['y']), jnp.array(test_data_15_extracted['a_coef'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for 5 Fourier basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_5_basis = generate_data(M = 1.0, N_fourier_basis=5, seed = 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a_fourier):\n",
    "    basis_fourier = jnp.array([jnp.sin(l * jnp.pi * x) for l in range(1,6)])\n",
    "    return jnp.dot(a_fourier, basis_fourier)\n",
    "\n",
    "MSE_test_3 = []\n",
    "for n_input_sensors in params_from_combined_loss.keys():\n",
    "    branch_layer_sizes = [n_input_sensors, 64, 64, 10]\n",
    "    trunk_layer_sizes = [2, 64, 64, 10]\n",
    "    x_sensors = np.linspace(0,1, n_input_sensors)\n",
    "    forcing_terms_test = []\n",
    "    nn = DeepONet(branch_layer_sizes, trunk_layer_sizes, random.PRNGKey(n_input_sensors))\n",
    "    nn.set_params(params_extracted[n_input_sensors])\n",
    "    for a in test_data_5_basis['a_coef']:\n",
    "        a_fourier = np.copy(a)\n",
    "        forcing_terms_test.append(f(x_sensors, a_fourier))\n",
    "\n",
    "    test_data_5_basis['f'] = forcing_terms_test\n",
    "    u_nn = jax.jit(lambda params, f, x, t : nn.forward(params, f, jnp.stack([x, t], axis=-1)))\n",
    "\n",
    "    u_nn_t = jax.grad(u_nn, argnums= 3)\n",
    "    u_nn_x = jax.grad(u_nn, argnums= 2)\n",
    "    u_nn_xx = jax.grad(u_nn_x, argnums= 2)\n",
    "    jit\n",
    "    def squared_error(params, f_sample, x, t, y, a):\n",
    "        return (u_nn(params, f_sample, x, t) - y)**2\n",
    "\n",
    "    squared_error_batch = jit(vmap(squared_error, in_axes =(None, 0, 0, 0, 0, 0))) \n",
    "    jit \n",
    "    def data_loss(params, f_sample, x, t, y, a):\n",
    "        return jnp.mean(squared_error_batch(params, f_sample, x, t, y,a))\n",
    "    print(n_input_sensors, data_loss(nn.params(), jnp.array(test_data_5_basis['f']), jnp.array(test_data_5_basis['x']), jnp.array(test_data_5_basis['t']),jnp.array(test_data_5_basis['y']), jnp.array(test_data_5_basis['a_coef'])))\n",
    "    MSE_test_3.append(data_loss(nn.params(), jnp.array(test_data_5_basis['f']), jnp.array(test_data_5_basis['x']), jnp.array(test_data_5_basis['t']),jnp.array(test_data_5_basis['y']), jnp.array(test_data_5_basis['a_coef'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "plt.title(f'Test loss, measured in MSE')\n",
    "plt.plot(params_from_combined_loss.keys(), MSE_test_1, 'r.--', label = r'$a_n \\leq 1.0, f(x) = \\sum_{i=1}^4 a_i \\ \\sin(i\\pi x)$')\n",
    "plt.plot(params_from_combined_loss.keys(), MSE_test_2, 'b.--', label = r'$a_n \\leq 1.5,  f(x) = \\sum_{i=1}^4 a_i \\ \\sin(i\\pi x)$')\n",
    "plt.plot(params_from_combined_loss.keys(), MSE_test_3, 'g.--', label = r'$a_n \\leq 1.0,  f(x) = \\sum_{i=1}^5 a_i \\ \\sin(i\\pi x)$')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.ylabel(r'MSE')\n",
    "plt.grid(True)\n",
    "plt.savefig(f'test_loss_input_sensors.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
